{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c28e86",
   "metadata": {},
   "source": [
    "# Week 1 ‚Äî LangChain Fundamentals & Basic Chains\n",
    "\n",
    "**Course:** LangChain for AI Applications  \n",
    "**Week Focus:** Understand LLMs, prompts, chains, and how to build your first AI applications.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this week, you will:\n",
    "- Understand LLM fundamentals and API interactions\n",
    "- Build prompt templates for consistent outputs\n",
    "- Create simple and sequential chains\n",
    "- Handle LLM responses and parse outputs\n",
    "- Build your first customer support chatbot\n",
    "- Deploy a basic LangChain application\n",
    "\n",
    "## üìä Real-World Context\n",
    "\n",
    "LangChain lets you build AI applications that:\n",
    "- **Automate tasks**: Customer support, data processing, content generation\n",
    "- **Augment workflows**: Combine LLMs with your tools and data\n",
    "- **Scale reasoning**: Chain multiple LLM calls for complex problems\n",
    "- **Maintain context**: Track conversation history and state\n",
    "\n",
    "Companies use LangChain to:\n",
    "- Build AI customer service agents (Intercom, Zendesk integration)\n",
    "- Automate document processing (PDFs ‚Üí insights)\n",
    "- Generate personalized content (emails, recommendations)\n",
    "- Create code assistants and debugging tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ec818",
   "metadata": {},
   "source": [
    "## üè¢ Scenario ‚Äî Build an AI Customer Support Assistant\n",
    "\n",
    "Your SaaS company receives 500+ support tickets daily. Your goal:\n",
    "1. Analyze customer issues automatically\n",
    "2. Draft responses using LLMs\n",
    "3. Route complex issues to humans\n",
    "4. Track conversation context\n",
    "\n",
    "We'll build this step-by-step using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64bd003",
   "metadata": {},
   "source": [
    "## üìö Key Concepts ‚Äî Why LangChain?\n",
    "\n",
    "### The Problem Without LangChain\n",
    "```python\n",
    "# Raw OpenAI API (verbose, error-prone)\n",
    "import openai\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a support agent\"},\n",
    "        {\"role\": \"user\", \"content\": \"My app is crashing\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    ")\n",
    "answer = response['choices'][0]['message']['content']\n",
    "```\n",
    "\n",
    "### The Solution With LangChain\n",
    "```python\n",
    "# LangChain (clean, composable, extensible)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a support agent. Help: {user_issue}\"\n",
    ")\n",
    "chain = prompt | llm\n",
    "answer = chain.invoke({\"user_issue\": \"My app is crashing\"})\n",
    "```\n",
    "\n",
    "### LangChain's Core Concepts\n",
    "1. **LLMs**: Language models (OpenAI, Anthropic, Llama, etc.)\n",
    "2. **Prompts**: Templates for consistent instructions\n",
    "3. **Chains**: Sequences of operations (prompt ‚Üí LLM ‚Üí parser)\n",
    "4. **Memory**: Tracking conversation history\n",
    "5. **Tools**: Integrations (databases, APIs, files)\n",
    "6. **Agents**: AI making decisions about which tools to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f08a104",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Hands-on Exercises\n",
    "\n",
    "### Exercise 1: Prompt Templates\n",
    "Create a prompt template for support tickets:\n",
    "- Input: customer_name, issue_type, issue_description\n",
    "- Output: helpful support response\n",
    "- Test with multiple customer scenarios\n",
    "\n",
    "### Exercise 2: Simple Chain\n",
    "Build a chain: Prompt ‚Üí LLM ‚Üí Output Parser\n",
    "- Use ChatOpenAI model\n",
    "- Parse structured output (JSON)\n",
    "- Handle multiple issue types\n",
    "\n",
    "### Exercise 3: Chain Pipeline\n",
    "Chain multiple operations:\n",
    "1. Analyze issue (classify as bug/feature/question)\n",
    "2. Generate response based on type\n",
    "3. Rate urgency (1-5)\n",
    "4. Return structured result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7634321c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint ‚Äî Building Your First Chain</summary>\n",
    "\n",
    "**Step 1: Install LangChain**\n",
    "```bash\n",
    "pip install langchain langchain-openai python-dotenv\n",
    "```\n",
    "\n",
    "**Step 2: Set up environment**\n",
    "```python\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load OPENAI_API_KEY from .env\n",
    "```\n",
    "\n",
    "**Step 3: Create LLM**\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "```\n",
    "\n",
    "**Step 4: Build prompt + chain**\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a support agent. {input}\"\n",
    ")\n",
    "chain = prompt | llm\n",
    "result = chain.invoke({\"input\": \"...user question...\"})\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc192c72",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>‚úÖ Solution ‚Äî Customer Support Chain</summary>\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define expected output structure\n",
    "class SupportResponse(BaseModel):\n",
    "    issue_type: str = Field(description=\"bug, feature, or question\")\n",
    "    response: str = Field(description=\"helpful support message\")\n",
    "    urgency: int = Field(description=\"1-5 scale\")\n",
    "    escalate: bool = Field(description=\"needs human review?\")\n",
    "\n",
    "# Create LLM and parser\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "parser = JsonOutputParser(pydantic_object=SupportResponse)\n",
    "\n",
    "# Build prompt with format instructions\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a SaaS support agent. Analyze this customer issue:\n",
    "    \n",
    "    Customer: {customer_name}\n",
    "    Issue: {issue_description}\n",
    "    \n",
    "    Provide a response with:\n",
    "    - Issue classification (bug/feature/question)\n",
    "    - Helpful response text\n",
    "    - Urgency level (1-5)\n",
    "    - Whether to escalate to human\n",
    "    \n",
    "    {format_instructions}\"\"\"\n",
    ")\n",
    "\n",
    "# Add format instructions\n",
    "prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# Build chain\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# Test\n",
    "result = chain.invoke({\n",
    "    \"customer_name\": \"Alice\",\n",
    "    \"issue_description\": \"The export button doesn't work\"\n",
    "})\n",
    "\n",
    "print(f\"Type: {result['issue_type']}\")\n",
    "print(f\"Response: {result['response']}\")\n",
    "print(f\"Urgency: {result['urgency']}\")\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- `ChatPromptTemplate`: Structured prompts with variable injection\n",
    "- `JsonOutputParser`: Automatic JSON parsing with validation\n",
    "- Pipe operator `|`: Clean chain composition (prompt ‚Üí LLM ‚Üí parser)\n",
    "- `pydantic`: Type-safe output validation\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00225397",
   "metadata": {},
   "source": [
    "## ü§î Reflection & Application\n",
    "\n",
    "**Question 1:** When should you use temperature=0.7 vs 0 vs 1?\n",
    "- temperature=0: Deterministic, best for classification and extraction\n",
    "- temperature=0.7: Balanced creativity and consistency (default good choice)\n",
    "- temperature=1.0: Creative, good for brainstorming and content generation\n",
    "\n",
    "**Question 2:** How do you prevent hallucinations?\n",
    "- Use structured outputs (JSON parsing)\n",
    "- Add \"Answer only based on provided information\"\n",
    "- Use few-shot examples in prompts\n",
    "- Verify against ground truth\n",
    "\n",
    "**Question 3:** When is a chain not enough?\n",
    "- When you need to make decisions (use agents)\n",
    "- When you need to retry/recover (add error handling)\n",
    "- When you need tool access (use agents with tools)\n",
    "\n",
    "## üìù Practice Assignment\n",
    "\n",
    "**Build:** An automated email classification system\n",
    "\n",
    "Requirements:\n",
    "1. Take email subject + body as input\n",
    "2. Classify: spam, urgent, normal, followup\n",
    "3. Extract action items (if any)\n",
    "4. Generate brief summary\n",
    "5. Test with 5 different emails\n",
    "\n",
    "## üîó Next Steps\n",
    "\n",
    "In Week 2, we'll add **memory** to track conversations and build **agents** that can use tools autonomously."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
