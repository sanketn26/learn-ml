{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6233c0d2",
   "metadata": {},
   "source": [
    "# Week 6 ‚Äî Production & Deployment\n",
    "\n",
    "**Course:** LangChain for AI Applications  \n",
    "**Week Focus:** Deploy LangChain applications to production at scale.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this week, you will:\n",
    "- Build scalable LangChain REST APIs with FastAPI\n",
    "- Deploy applications with Docker\n",
    "- Scale to cloud platforms (AWS, GCP, Azure)\n",
    "- Handle rate limiting and request queues\n",
    "- Monitor and maintain production systems\n",
    "- Implement caching and optimization\n",
    "\n",
    "## üìä Real-World Context\n",
    "\n",
    "**The Challenge:**\n",
    "- Your support bot works perfectly in development\n",
    "- Now you need to serve 1000 concurrent users\n",
    "- Handle peak loads (Black Friday = 10x traffic)\n",
    "- Keep costs reasonable ($$ per request)\n",
    "- Maintain 99.9% uptime\n",
    "\n",
    "**Production Concerns:**\n",
    "1. **Performance:** Respond in < 2 seconds at 1000 RPS\n",
    "2. **Cost:** Optimize token usage ($$ adds up fast)\n",
    "3. **Reliability:** Handle failures gracefully\n",
    "4. **Scalability:** Auto-scale with traffic\n",
    "5. **Monitoring:** Know what's happening in production\n",
    "6. **Security:** Protect API, data, credentials\n",
    "\n",
    "**Solutions:**\n",
    "- Async chains and FastAPI for performance\n",
    "- Caching to reduce API calls\n",
    "- Request queuing for load smoothing\n",
    "- Circuit breakers for resilience\n",
    "- Containerization with Docker\n",
    "- Cloud deployment with auto-scaling\n",
    "- Comprehensive monitoring and logging\n",
    "\n",
    "**Business Impact:**\n",
    "- üìà Scale: Handle growth without rewrite\n",
    "- üí∞ Cost: 50% reduction via caching\n",
    "- ‚ö° Speed: < 500ms response time\n",
    "- üîí Reliability: 99.9% uptime\n",
    "- üëÄ Visibility: Real-time monitoring\n",
    "- üöÄ Faster deployments: CI/CD pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aed174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''\n",
    "<style>\n",
    ".api-box {\n",
    "    background-color: #e3f2fd;\n",
    "    border-left: 5px solid #2196f3;\n",
    "    padding: 15px;\n",
    "    margin: 20px 0;\n",
    "    border-radius: 5px;\n",
    "}\n",
    ".scale-box {\n",
    "    background-color: #f3e5f5;\n",
    "    border-left: 5px solid #9c27b0;\n",
    "    padding: 15px;\n",
    "    margin: 20px 0;\n",
    "    border-radius: 5px;\n",
    "}\n",
    ".exercise-box {\n",
    "    background-color: #fff3cd;\n",
    "    border-left: 5px solid #ffc107;\n",
    "    padding: 15px;\n",
    "    margin: 20px 0;\n",
    "    border-radius: 5px;\n",
    "}\n",
    "</style>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43397c9",
   "metadata": {},
   "source": [
    "## üöÄ Part 1: Building REST APIs with FastAPI\n",
    "\n",
    "<div class=\"api-box\">\n",
    "<strong>FastAPI:</strong> Modern, fast Python web framework for building production-ready APIs.\n",
    "</div>\n",
    "\n",
    "### Why FastAPI?\n",
    "\n",
    "| Feature | FastAPI | Flask | Django |\n",
    "|---------|---------|-------|--------|\n",
    "| Speed | ‚ö°‚ö°‚ö° Fastest | ‚ö° Good | ‚ö° Good |\n",
    "| Async | Native | Limited | Limited |\n",
    "| Validation | Auto | Manual | Manual |\n",
    "| Docs | Auto | Manual | Manual |\n",
    "| Learning | Easy | Easy | Steep |\n",
    "\n",
    "### FastAPI Example\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    context: str = None\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    response: str\n",
    "    latency_ms: float\n",
    "\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat(request: ChatRequest):\n",
    "    \"\"\"Process chat message with LangChain.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    # Your LangChain logic here\n",
    "    response = await llm.agenerate(request.message)\n",
    "    \n",
    "    latency = (time.time() - start) * 1000\n",
    "    return ChatResponse(response=response, latency_ms=latency)\n",
    "\n",
    "# Run with: uvicorn main:app --reload\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55adffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate API structure\n",
    "\n",
    "from typing import Optional, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "@dataclass\n",
    "class APIRequest:\n",
    "    \"\"\"Incoming API request.\"\"\"\n",
    "    request_id: str\n",
    "    endpoint: str\n",
    "    message: str\n",
    "    timestamp: datetime\n",
    "    user_id: str\n",
    "\n",
    "@dataclass\n",
    "class APIResponse:\n",
    "    \"\"\"Outgoing API response.\"\"\"\n",
    "    request_id: str\n",
    "    response: str\n",
    "    latency_ms: float\n",
    "    model: str\n",
    "    tokens_used: int\n",
    "    cached: bool\n",
    "\n",
    "class SimpleCache:\n",
    "    \"\"\"Simple LRU cache for responses.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 100):\n",
    "        self.cache: Dict[str, str] = {}\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    def get(self, key: str) -> Optional[str]:\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def set(self, key: str, value: str):\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            # Simple FIFO eviction\n",
    "            self.cache.pop(next(iter(self.cache)))\n",
    "        self.cache[key] = value\n",
    "    \n",
    "    def stats(self) -> Dict[str, int]:\n",
    "        return {\"cached_items\": len(self.cache), \"max_size\": self.max_size}\n",
    "\n",
    "# Demo: API structure\n",
    "print(\"üîå FASTAPI STRUCTURE DEMO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cache = SimpleCache(max_size=5)\n",
    "\n",
    "# Simulate requests\n",
    "print(\"\\nüìù Processing Requests:\")\n",
    "print()\n",
    "\n",
    "# Request 1: Cache miss\n",
    "req1 = APIRequest(\n",
    "    request_id=\"req-001\",\n",
    "    endpoint=\"/chat\",\n",
    "    message=\"How do I reset password?\",\n",
    "    timestamp=datetime.now(),\n",
    "    user_id=\"user-123\"\n",
    ")\n",
    "\n",
    "cached = cache.get(req1.message)\n",
    "if cached:\n",
    "    print(f\"1. {req1.request_id}: CACHE HIT\")\n",
    "    print(f\"   Response: {cached}\")\n",
    "    print(f\"   Latency: 1ms (cached)\")\n",
    "    cache.set(req1.message, \"Go to Settings > Security > Change Password\")\n",
    "else:\n",
    "    print(f\"1. {req1.request_id}: CACHE MISS\")\n",
    "    print(f\"   Message: {req1.message}\")\n",
    "    resp = \"Go to Settings > Security > Change Password\"\n",
    "    cache.set(req1.message, resp)\n",
    "    print(f\"   Response: {resp}\")\n",
    "    print(f\"   Latency: 850ms (API call)\")\n",
    "    print(f\"   Tokens: 45\")\n",
    "\n",
    "# Request 2: Same question = cache hit\n",
    "print(f\"\\n2. req-002: CACHE HIT\")\n",
    "print(f\"   Message: {req1.message}\")\n",
    "print(f\"   Response: {cache.get(req1.message)}\")\n",
    "print(f\"   Latency: 2ms (cached)\")\n",
    "print(f\"   Tokens: 0 (SAVED!)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"\\nüíæ Cache Statistics:\")\n",
    "stats = cache.stats()\n",
    "print(f\"  Items cached: {stats['cached_items']}/{stats['max_size']}\")\n",
    "print(f\"  ‚úÖ Benefit: Request 2 was 400x faster and cost-free!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e7214",
   "metadata": {},
   "source": [
    "## üê≥ Part 2: Docker Containerization\n",
    "\n",
    "### Dockerfile Example\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application\n",
    "COPY app/ .\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s \\\n",
    "  CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Run server\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "### Build & Run\n",
    "\n",
    "```bash\n",
    "# Build image\n",
    "docker build -t langchain-app:v1 .\n",
    "\n",
    "# Run container\n",
    "docker run -p 8000:8000 langchain-app:v1\n",
    "\n",
    "# Push to registry\n",
    "docker push myregistry.azurecr.io/langchain-app:v1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90669a8",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Part 3: Cloud Deployment\n",
    "\n",
    "<div class=\"scale-box\">\n",
    "<strong>Cloud Deployment:</strong> Running containers at scale on managed platforms.\n",
    "</div>\n",
    "\n",
    "### Deployment Options\n",
    "\n",
    "| Platform | Setup | Scaling | Cost | Best For |\n",
    "|----------|-------|---------|------|----------|\n",
    "| **AWS ECS** | Medium | Auto | Pay-per-use | High scale |\n",
    "| **Google Cloud Run** | Easy | Auto | Pay-per-request | Unpredictable |\n",
    "| **Azure Container Instances** | Medium | Manual | Hourly | Predictable |\n",
    "| **Heroku** | Very Easy | Auto | Fixed | Rapid prototyping |\n",
    "| **Kubernetes** | Hard | Auto | Flexible | Enterprise |\n",
    "\n",
    "### Scaling Strategy\n",
    "\n",
    "```\n",
    "Load Balancer\n",
    "    ‚Üì\n",
    "[Instance 1] [Instance 2] [Instance 3]\n",
    "    ‚Üì           ‚Üì           ‚Üì\n",
    "[Cache] [Cache] [Cache]\n",
    "    ‚Üì           ‚Üì           ‚Üì\n",
    "[Queue] [Queue] [Queue]\n",
    "    ‚Üì           ‚Üì           ‚Üì\n",
    "           [LLM API]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02abde9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate load balancing and scaling\n",
    "\n",
    "from collections import deque\n",
    "import statistics\n",
    "\n",
    "class LoadBalancer:\n",
    "    \"\"\"Distribute requests across multiple instances.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_instances: int):\n",
    "        self.instances = [f\"instance-{i}\" for i in range(num_instances)]\n",
    "        self.request_queue = deque()\n",
    "        self.current_instance = 0\n",
    "        self.request_count = {inst: 0 for inst in self.instances}\n",
    "        self.latencies = {inst: [] for inst in self.instances}\n",
    "    \n",
    "    def route_request(self, request_id: str) -> str:\n",
    "        \"\"\"Route to least-loaded instance (round-robin).\"\"\"\n",
    "        instance = self.instances[self.current_instance]\n",
    "        self.current_instance = (self.current_instance + 1) % len(self.instances)\n",
    "        \n",
    "        self.request_count[instance] += 1\n",
    "        return instance\n",
    "    \n",
    "    def record_latency(self, instance: str, latency_ms: float):\n",
    "        \"\"\"Record response latency for monitoring.\"\"\"\n",
    "        self.latencies[instance].append(latency_ms)\n",
    "    \n",
    "    def should_scale_up(self) -> bool:\n",
    "        \"\"\"Check if we should add more instances.\"\"\"\n",
    "        if not self.latencies[self.instances[0]]:\n",
    "            return False\n",
    "        \n",
    "        avg_latency = statistics.mean(self.latencies[self.instances[0]])\n",
    "        return avg_latency > 1500  # Threshold: 1.5s\n",
    "    \n",
    "    def scale_up(self):\n",
    "        \"\"\"Add a new instance.\"\"\"\n",
    "        new_instance = f\"instance-{len(self.instances)}\"\n",
    "        self.instances.append(new_instance)\n",
    "        self.request_count[new_instance] = 0\n",
    "        self.latencies[new_instance] = []\n",
    "        return new_instance\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get load balancing statistics.\"\"\"\n",
    "        total_requests = sum(self.request_count.values())\n",
    "        \n",
    "        stats = {\n",
    "            \"total_instances\": len(self.instances),\n",
    "            \"total_requests\": total_requests,\n",
    "            \"by_instance\": self.request_count.copy(),\n",
    "        }\n",
    "        \n",
    "        # Calculate latency stats\n",
    "        all_latencies = []\n",
    "        for lat_list in self.latencies.values():\n",
    "            all_latencies.extend(lat_list)\n",
    "        \n",
    "        if all_latencies:\n",
    "            stats[\"avg_latency_ms\"] = round(statistics.mean(all_latencies), 1)\n",
    "            stats[\"p95_latency_ms\"] = round(\n",
    "                sorted(all_latencies)[int(len(all_latencies) * 0.95)], 1\n",
    "            )\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Demo: Load balancing and scaling\n",
    "print(\"‚öñÔ∏è  LOAD BALANCING & SCALING DEMO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lb = LoadBalancer(num_instances=2)\n",
    "\n",
    "# Simulate 20 requests\n",
    "print(\"\\nüìä Handling Incoming Requests:\")\n",
    "print()\n",
    "\n",
    "for i in range(10):\n",
    "    instance = lb.route_request(f\"req-{i:03d}\")\n",
    "    latency = 800 + (i * 100)  # Increasing latency\n",
    "    lb.record_latency(instance, latency)\n",
    "    print(f\"Request {i+1:2d} ‚Üí {instance} (latency: {latency}ms)\")\n",
    "    \n",
    "    if lb.should_scale_up():\n",
    "        new_instance = lb.scale_up()\n",
    "        print(f\"  üî∫ SCALING UP: Added {new_instance}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"\\nüìà LOAD BALANCER STATISTICS:\")\n",
    "stats = lb.get_stats()\n",
    "for key, value in stats.items():\n",
    "    if key == \"by_instance\":\n",
    "        print(f\"  {key}:\")\n",
    "        for inst, count in value.items():\n",
    "            print(f\"    - {inst}: {count} requests\")\n",
    "    else:\n",
    "        print(f\"  {key:20} {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ee66f",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Hands-On Exercises\n",
    "\n",
    "<div class=\"exercise-box\">\n",
    "<strong>üéØ Exercise 1: Build FastAPI Server</strong><br><br>\n",
    "Create a production-ready API:\n",
    "<ol>\n",
    "<li>Define request/response models</li>\n",
    "<li>Implement async handlers</li>\n",
    "<li>Add error handling and validation</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c924a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your FastAPI server here!\n",
    "print(\"Your production FastAPI server implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42e4bee",
   "metadata": {},
   "source": [
    "<div class=\"exercise-box\">\n",
    "<strong>üéØ Exercise 2: Create Dockerfile & Deploy</strong><br><br>\n",
    "Containerize and deploy:\n",
    "<ol>\n",
    "<li>Write Dockerfile with best practices</li>\n",
    "<li>Build and test locally</li>\n",
    "<li>Push to Docker registry</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bff3536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your Docker deployment here!\n",
    "print(\"Your Dockerfile and deployment script!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7969c2",
   "metadata": {},
   "source": [
    "## üìù Week 6 Project: Production Deployment\n",
    "\n",
    "**Deploy a complete LangChain application to production with full monitoring.**\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "**1. FastAPI Server:**\n",
    "- `/chat` endpoint (POST)\n",
    "- `/health` endpoint for monitoring\n",
    "- Input validation with Pydantic\n",
    "- Async request handling\n",
    "- Error handling & retries\n",
    "\n",
    "**2. Caching:**\n",
    "- LRU cache for common queries\n",
    "- Reduce API calls by 50%+\n",
    "- Track cache hit rate\n",
    "\n",
    "**3. Docker Setup:**\n",
    "- Optimized Dockerfile\n",
    "- Multi-stage builds\n",
    "- Health checks\n",
    "- Environment variables\n",
    "\n",
    "**4. Load Testing:**\n",
    "- Test with 100+ concurrent users\n",
    "- Measure response times\n",
    "- Identify bottlenecks\n",
    "\n",
    "**5. Monitoring:**\n",
    "- Request/response logging\n",
    "- Performance metrics\n",
    "- Error tracking\n",
    "- Uptime monitoring\n",
    "\n",
    "### Deliverables:\n",
    "- main.py (FastAPI app)\n",
    "- requirements.txt (dependencies)\n",
    "- Dockerfile (containerization)\n",
    "- docker-compose.yml (local testing)\n",
    "- load_test.py (performance testing)\n",
    "- deployment_guide.md (cloud deployment)\n",
    "- monitoring_dashboard.md (production metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5523d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 6 Project Starter\n",
    "\n",
    "# TODO: Build FastAPI server with async handlers\n",
    "# TODO: Implement request caching\n",
    "# TODO: Create Docker setup\n",
    "# TODO: Write load testing script\n",
    "# TODO: Set up monitoring\n",
    "# TODO: Deploy to cloud platform\n",
    "# TODO: Document deployment process\n",
    "\n",
    "print(\"üéØ Your complete production deployment here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db32e62",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "**What you learned this week:**\n",
    "\n",
    "‚úÖ **REST APIs:**\n",
    "- FastAPI for high-performance servers\n",
    "- Async request handling\n",
    "- Automatic validation & documentation\n",
    "\n",
    "‚úÖ **Containerization:**\n",
    "- Docker for reproducible deployments\n",
    "- Multi-stage builds for optimization\n",
    "- Health checks for reliability\n",
    "\n",
    "‚úÖ **Cloud Deployment:**\n",
    "- Scaling strategies\n",
    "- Load balancing\n",
    "- Auto-scaling policies\n",
    "\n",
    "‚úÖ **Production Operations:**\n",
    "- Monitoring and logging\n",
    "- Performance optimization\n",
    "- Cost management\n",
    "- Continuous deployment\n",
    "\n",
    "## üèÜ Capstone: Your Complete LangChain Mastery\n",
    "\n",
    "**You've now mastered the complete LangChain journey:**\n",
    "\n",
    "- ‚úÖ Week 1-2: Fundamentals & memory\n",
    "- ‚úÖ Week 3: Agents & tools\n",
    "- ‚úÖ Week 4: RAG & embeddings\n",
    "- ‚úÖ Week 5: Evaluation & debugging\n",
    "- ‚úÖ Week 6: Production & deployment\n",
    "\n",
    "**Build your final capstone project:**\n",
    "- A complete, production-ready LLM application\n",
    "- Tested, evaluated, and monitored\n",
    "- Deployed and scaling in the cloud\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations on completing LangChain Mastery!** You're now ready to build production LLM applications. üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
