{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f48519",
   "metadata": {},
   "source": [
    "# Week 5 â€” Evaluation & Debugging\n",
    "\n",
    "**Course:** LangChain for AI Applications  \n",
    "**Week Focus:** Test, evaluate, and debug LangChain applications scientifically.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this week, you will:\n",
    "- Design and implement evaluation metrics for LLM outputs\n",
    "- Use LangSmith for distributed tracing and debugging\n",
    "- Build comprehensive test datasets and suites\n",
    "- Handle edge cases and failure modes\n",
    "- Monitor application performance in production\n",
    "- Identify and fix hallucinations and errors\n",
    "\n",
    "## ğŸ“Š Real-World Context\n",
    "\n",
    "**The Problem:**\n",
    "- Your support bot works 95% of the time\n",
    "- But what about the other 5%?\n",
    "- How do you know which cases fail?\n",
    "- How do you fix them systematically?\n",
    "\n",
    "**Failures to Catch:**\n",
    "1. **Hallucinations:** Confident but wrong answers\n",
    "2. **Wrong Tool Selection:** Using search when it should escalate\n",
    "3. **Slow Responses:** Timeouts or excessive latency\n",
    "4. **Missing Context:** Insufficient information retrieved\n",
    "5. **Escalation Failures:** Not recognizing when to hand off to humans\n",
    "\n",
    "**Solutions:**\n",
    "- Define quantitative metrics (accuracy, relevance, latency)\n",
    "- Trace execution with LangSmith\n",
    "- Create golden datasets for regression testing\n",
    "- Monitor production continuously\n",
    "- Build feedback loops from user interactions\n",
    "\n",
    "**Business Impact:**\n",
    "- ğŸ“ˆ Quality: Catch issues before users do (95% â†’ 99%+)\n",
    "- ğŸ” Visibility: Understand exactly what went wrong\n",
    "- âš¡ Speed: Debug in minutes, not days\n",
    "- ğŸ’° Cost: Reduce wasted API calls on failures\n",
    "- ğŸ“Š Continuous improvement: Data-driven optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309c1601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''\n",
    "<style>\n",
    ".metric-box {\n",
    "    background-color: #e8f5e9;\n",
    "    border-left: 5px solid #4caf50;\n",
    "    padding: 15px;\n",
    "    margin: 20px 0;\n",
    "    border-radius: 5px;\n",
    "}\n",
    ".debug-box {\n",
    "    background-color: #fff3e0;\n",
    "    border-left: 5px solid #ff9800;\n",
    "    padding: 15px;\n",
    "    margin: 20px 0;\n",
    "    border-radius: 5px;\n",
    "}\n",
    ".exercise-box {\n",
    "    background-color: #fff3cd;\n",
    "    border-left: 5px solid #ffc107;\n",
    "    padding: 15px;\n",
    "    margin: 20px 0;\n",
    "    border-radius: 5px;\n",
    "}\n",
    "</style>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a0b8c4",
   "metadata": {},
   "source": [
    "## ğŸ” Part 1: Evaluation Metrics\n",
    "\n",
    "<div class=\"metric-box\">\n",
    "<strong>Evaluation Metric:</strong> A quantitative measure of LLM application quality.\n",
    "</div>\n",
    "\n",
    "### Key Metrics for LLM Applications\n",
    "\n",
    "| Metric | Measures | Example |\n",
    "|--------|----------|----------|\n",
    "| **Accuracy** | % correct answers | Support bot: Does it solve the problem? |\n",
    "| **Relevance** | Info matches query | Retrieval: Are results on-topic? |\n",
    "| **Latency** | Response time | API: Respond in < 2 seconds |\n",
    "| **Toxicity** | Harmful language | Safety: Flag inappropriate content |\n",
    "| **Hallucination** | False confidence | Fact-checking: Verify claims |\n",
    "| **Tool Use** | Correct tool selection | Agents: Use right tool first try? |\n",
    "\n",
    "### Ground Truth Datasets\n",
    "\n",
    "You need a **golden dataset** of known good outputs:\n",
    "\n",
    "```python\n",
    "test_cases = [\n",
    "    {\n",
    "        \"input\": \"How do I reset my password?\",\n",
    "        \"expected_output\": \"Go to Settings > Security > Change Password\",\n",
    "        \"expected_tool\": \"documentation_search\",\n",
    "        \"max_latency_ms\": 2000,\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I'm extremely angry about my billing\",\n",
    "        \"expected_output\": \"Escalate to human agent\",\n",
    "        \"expected_tool\": \"escalate_to_human\",\n",
    "        \"max_latency_ms\": 500,\n",
    "    },\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971d6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement evaluation system\n",
    "\n",
    "from typing import Dict, List, Any\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import time\n",
    "\n",
    "class EvaluationResult(Enum):\n",
    "    PASS = \"pass\"\n",
    "    FAIL = \"fail\"\n",
    "    PARTIAL = \"partial\"\n",
    "\n",
    "@dataclass\n",
    "class TestCase:\n",
    "    \"\"\"Represents a single test case for evaluation.\"\"\"\n",
    "    input_query: str\n",
    "    expected_output: str\n",
    "    expected_tool: str\n",
    "    max_latency_ms: int = 2000\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "@dataclass\n",
    "class EvaluationScore:\n",
    "    \"\"\"Stores evaluation results for a test run.\"\"\"\n",
    "    test_case: TestCase\n",
    "    actual_output: str\n",
    "    actual_tool: str\n",
    "    latency_ms: float\n",
    "    result: EvaluationResult\n",
    "    accuracy_score: float  # 0-1\n",
    "    relevance_score: float  # 0-1\n",
    "    reasoning: str\n",
    "\n",
    "class ApplicationEvaluator:\n",
    "    \"\"\"Evaluate LLM application quality systematically.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.test_results: List[EvaluationScore] = []\n",
    "    \n",
    "    def evaluate_output_match(self, expected: str, actual: str) -> float:\n",
    "        \"\"\"Calculate how closely actual matches expected (0-1).\"\"\"\n",
    "        # Simple word overlap scoring\n",
    "        expected_words = set(expected.lower().split())\n",
    "        actual_words = set(actual.lower().split())\n",
    "        \n",
    "        if not expected_words:\n",
    "            return 1.0 if not actual_words else 0.0\n",
    "        \n",
    "        overlap = len(expected_words & actual_words)\n",
    "        return overlap / len(expected_words)\n",
    "    \n",
    "    def evaluate_tool_selection(self, expected: str, actual: str) -> bool:\n",
    "        \"\"\"Check if correct tool was selected.\"\"\"\n",
    "        return expected.lower() == actual.lower()\n",
    "    \n",
    "    def evaluate_latency(self, latency_ms: float, max_ms: int) -> bool:\n",
    "        \"\"\"Check if latency is within SLA.\"\"\"\n",
    "        return latency_ms <= max_ms\n",
    "    \n",
    "    def run_test(self, test_case: TestCase, actual_output: str, \n",
    "                actual_tool: str, latency_ms: float) -> EvaluationScore:\n",
    "        \"\"\"Run a single test case evaluation.\"\"\"\n",
    "        \n",
    "        # Score accuracy\n",
    "        accuracy = self.evaluate_output_match(test_case.expected_output, actual_output)\n",
    "        \n",
    "        # Score tool selection\n",
    "        tool_correct = self.evaluate_tool_selection(test_case.expected_tool, actual_tool)\n",
    "        \n",
    "        # Score latency\n",
    "        latency_ok = self.evaluate_latency(latency_ms, test_case.max_latency_ms)\n",
    "        \n",
    "        # Determine overall result\n",
    "        if tool_correct and accuracy > 0.8 and latency_ok:\n",
    "            result = EvaluationResult.PASS\n",
    "        elif tool_correct or accuracy > 0.6:\n",
    "            result = EvaluationResult.PARTIAL\n",
    "        else:\n",
    "            result = EvaluationResult.FAIL\n",
    "        \n",
    "        reasoning = f\"Accuracy: {accuracy:.1%}, Tool: {'âœ“' if tool_correct else 'âœ—'}, Latency: {latency_ms:.0f}ms\"\n",
    "        \n",
    "        score = EvaluationScore(\n",
    "            test_case=test_case,\n",
    "            actual_output=actual_output,\n",
    "            actual_tool=actual_tool,\n",
    "            latency_ms=latency_ms,\n",
    "            result=result,\n",
    "            accuracy_score=accuracy,\n",
    "            relevance_score=1.0 if latency_ok else 0.5,\n",
    "            reasoning=reasoning\n",
    "        )\n",
    "        \n",
    "        self.test_results.append(score)\n",
    "        return score\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get evaluation summary statistics.\"\"\"\n",
    "        if not self.test_results:\n",
    "            return {\"error\": \"No test results\"}\n",
    "        \n",
    "        passes = sum(1 for r in self.test_results if r.result == EvaluationResult.PASS)\n",
    "        partials = sum(1 for r in self.test_results if r.result == EvaluationResult.PARTIAL)\n",
    "        fails = sum(1 for r in self.test_results if r.result == EvaluationResult.FAIL)\n",
    "        \n",
    "        avg_accuracy = sum(r.accuracy_score for r in self.test_results) / len(self.test_results)\n",
    "        avg_latency = sum(r.latency_ms for r in self.test_results) / len(self.test_results)\n",
    "        \n",
    "        return {\n",
    "            \"total_tests\": len(self.test_results),\n",
    "            \"passed\": passes,\n",
    "            \"partial\": partials,\n",
    "            \"failed\": fails,\n",
    "            \"pass_rate\": f\"{100 * passes / len(self.test_results):.0f}%\",\n",
    "            \"avg_accuracy\": f\"{100 * avg_accuracy:.0f}%\",\n",
    "            \"avg_latency_ms\": f\"{avg_latency:.0f}ms\"\n",
    "        }\n",
    "\n",
    "# Demo: Evaluation in action\n",
    "print(\"ğŸ“Š EVALUATION SYSTEM DEMO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "evaluator = ApplicationEvaluator()\n",
    "\n",
    "# Create test cases\n",
    "test_cases = [\n",
    "    TestCase(\n",
    "        input_query=\"How do I reset my password?\",\n",
    "        expected_output=\"Go to Settings > Security > Change Password\",\n",
    "        expected_tool=\"documentation_search\",\n",
    "        max_latency_ms=2000\n",
    "    ),\n",
    "    TestCase(\n",
    "        input_query=\"I'm extremely angry about billing\",\n",
    "        expected_output=\"Escalate to human support\",\n",
    "        expected_tool=\"escalate_to_human\",\n",
    "        max_latency_ms=500\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Simulate app responses\n",
    "print(\"\\nğŸ§ª Running Test Suite...\\n\")\n",
    "\n",
    "# Test 1: Good response\n",
    "result1 = evaluator.run_test(\n",
    "    test_cases[0],\n",
    "    actual_output=\"Go to Settings, then Security, then Change Password\",\n",
    "    actual_tool=\"documentation_search\",\n",
    "    latency_ms=1250\n",
    ")\n",
    "print(f\"Test 1: {result1.result.value.upper()}\")\n",
    "print(f\"  {result1.reasoning}\")\n",
    "\n",
    "# Test 2: Slow escalation\n",
    "result2 = evaluator.run_test(\n",
    "    test_cases[1],\n",
    "    actual_output=\"Escalating to our support team\",\n",
    "    actual_tool=\"escalate_to_human\",\n",
    "    latency_ms=3500  # Too slow!\n",
    ")\n",
    "print(f\"\\nTest 2: {result2.result.value.upper()}\")\n",
    "print(f\"  {result2.reasoning}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"\\nğŸ“ˆ SUMMARY:\")\n",
    "summary = evaluator.get_summary()\n",
    "for key, value in summary.items():\n",
    "    print(f\"  {key:20} {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bde0b5",
   "metadata": {},
   "source": [
    "## ğŸ› Part 2: Debugging with Tracing\n",
    "\n",
    "<div class=\"debug-box\">\n",
    "<strong>Tracing:</strong> Recording detailed execution path of LLM application for debugging.\n",
    "</div>\n",
    "\n",
    "### Execution Trace Example\n",
    "\n",
    "```\n",
    "User Query: \"Help with my order\"\n",
    "  â”œâ”€ ğŸ¯ Router: Determine category\n",
    "  â”‚  â””â”€ LLM: \"This is about orders\"\n",
    "  â”‚  â””â”€ Selected Tool: order_lookup\n",
    "  â”‚  â””â”€ Latency: 125ms\n",
    "  â”‚\n",
    "  â”œâ”€ ğŸ” Order Lookup Tool\n",
    "  â”‚  â””â”€ Query: \"SELECT * FROM orders WHERE user_id=123\"\n",
    "  â”‚  â””â”€ Result: Order #ORD456 found\n",
    "  â”‚  â””â”€ Latency: 45ms\n",
    "  â”‚\n",
    "  â””â”€ ğŸ“ Response Generator\n",
    "     â””â”€ Template: \"Your order {{order_id}} is {{status}}\"\n",
    "     â””â”€ Output: \"Your order ORD456 is shipped\"\n",
    "     â””â”€ Latency: 87ms\n",
    "\n",
    "Total Time: 257ms âœ“\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2fba49",
   "metadata": {},
   "source": [
    "## ğŸ¯ Part 3: Handling Edge Cases\n",
    "\n",
    "### Common Failure Modes\n",
    "\n",
    "**1. Hallucinations**\n",
    "```\n",
    "Input: \"What's in my account?\"\n",
    "AI Response: \"Your account has $999.99\" (âŒ We have no data for this user)\n",
    "Fix: Validate AI output against retrieved data\n",
    "```\n",
    "\n",
    "**2. Incorrect Tool Selection**\n",
    "```\n",
    "Input: \"I'm canceling my subscription\"\n",
    "AI chooses: \"documentation_search\" (âŒ Should be \"request_cancellation\")\n",
    "Fix: Validate tool selection matches intent\n",
    "```\n",
    "\n",
    "**3. Timeout/Slow Responses**\n",
    "```\n",
    "Input: \"Analyze my usage data\"\n",
    "Response time: 8 seconds (âŒ SLA is 2 seconds)\n",
    "Fix: Add circuit breaker, fallback to summary\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2673dcb",
   "metadata": {},
   "source": [
    "## âœï¸ Hands-On Exercises\n",
    "\n",
    "<div class=\"exercise-box\">\n",
    "<strong>ğŸ¯ Exercise 1: Build Test Dataset</strong><br><br>\n",
    "Create a comprehensive test suite:\n",
    "<ol>\n",
    "<li>Design 10 test cases covering normal + edge cases</li>\n",
    "<li>Define expected outputs and SLAs</li>\n",
    "<li>Track ground truth data</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa0f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your test dataset here!\n",
    "print(\"Your comprehensive test dataset implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f0635",
   "metadata": {},
   "source": [
    "<div class=\"exercise-box\">\n",
    "<strong>ğŸ¯ Exercise 2: Implement Hallucination Detector</strong><br><br>\n",
    "Detect when AI is making up answers:\n",
    "<ol>\n",
    "<li>Compare AI output against retrieved facts</li>\n",
    "<li>Flag low-confidence claims</li>\n",
    "<li>Escalate uncertain responses</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fd781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your hallucination detector here!\n",
    "print(\"Your hallucination detection system!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d2ec76",
   "metadata": {},
   "source": [
    "## ğŸ“ Week 5 Project: Evaluation Suite\n",
    "\n",
    "**Build a complete evaluation and debugging system for an LLM application.**\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "**1. Test Dataset Creation:**\n",
    "- Create 50+ test cases covering:\n",
    "  - Normal happy paths\n",
    "  - Edge cases\n",
    "  - Error conditions\n",
    "  - Performance limits\n",
    "\n",
    "**2. Evaluation Metrics:**\n",
    "- Accuracy: Does output match expected?\n",
    "- Relevance: Is information on-topic?\n",
    "- Latency: Response time vs SLA\n",
    "- Tool correctness: Right tool selected?\n",
    "- Hallucination detection: Confident but wrong?\n",
    "\n",
    "**3. Execution Tracing:**\n",
    "- Log every step of execution\n",
    "- Record timestamps and latencies\n",
    "- Capture intermediate outputs\n",
    "\n",
    "**4. Reporting:**\n",
    "- Pass/fail summary\n",
    "- Failure analysis\n",
    "- Performance statistics\n",
    "- Recommendations for improvement\n",
    "\n",
    "### Deliverables:\n",
    "- test_cases.json with 50+ comprehensive tests\n",
    "- evaluator.py implementing metrics\n",
    "- evaluation_report.md with findings\n",
    "- recommendations.md with fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b62cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 5 Project Starter\n",
    "\n",
    "# TODO: Create comprehensive test dataset (50+ cases)\n",
    "# TODO: Implement evaluation metrics (accuracy, relevance, latency, hallucination)\n",
    "# TODO: Build tracing system\n",
    "# TODO: Generate evaluation report with findings\n",
    "# TODO: Create recommendations for improvement\n",
    "\n",
    "print(\"ğŸ¯ Your comprehensive evaluation suite here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356bbdda",
   "metadata": {},
   "source": [
    "## ğŸ“ Key Takeaways\n",
    "\n",
    "**What you learned this week:**\n",
    "\n",
    "âœ… **Evaluation Metrics:**\n",
    "- Accuracy, relevance, latency, hallucination\n",
    "- How to measure LLM quality quantitatively\n",
    "\n",
    "âœ… **Test Datasets:**\n",
    "- Golden datasets for regression testing\n",
    "- Edge case coverage\n",
    "\n",
    "âœ… **Debugging:**\n",
    "- Tracing execution paths\n",
    "- Finding failure root causes\n",
    "\n",
    "âœ… **Continuous Improvement:**\n",
    "- Feedback loops from testing\n",
    "- Data-driven optimization\n",
    "\n",
    "## ğŸ”œ Next Week: Production & Deployment\n",
    "\n",
    "In Week 6, we'll take your tested, debugged applications and deploy them at scale:\n",
    "- FastAPI servers\n",
    "- Docker containerization\n",
    "- Cloud deployment (AWS, GCP, Azure)\n",
    "- Load balancing and scaling\n",
    "- Monitoring in production\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Great job on Week 5!** Your applications are now bulletproof and debuggable. See you next week! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
