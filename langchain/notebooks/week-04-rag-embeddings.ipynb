{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d858751b",
   "metadata": {},
   "source": [
    "# Week 4 ‚Äî RAG & Embeddings\n",
    "\n",
    "**Course:** LangChain for AI Applications  \n",
    "**Week Focus:** Retrieval-Augmented Generation (RAG) - ground LLMs in your own data.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this week, you will:\n",
    "- Understand embeddings and why they work\n",
    "- Build and query vector stores efficiently\n",
    "- Implement document loading and chunking strategies\n",
    "- Create RAG systems that ground LLMs in real data\n",
    "- Combine retrieval with generation for accurate answers\n",
    "- Build a knowledge-grounded customer support system\n",
    "\n",
    "## üìä Real-World Context\n",
    "\n",
    "**The Problem:** Your SaaS company has 500+ pages of documentation:\n",
    "- API guides, tutorials, troubleshooting docs\n",
    "- Product specs, pricing guides, FAQs\n",
    "- Support runbooks, internal procedures\n",
    "\n",
    "**Challenges:**\n",
    "- LLMs hallucinate answers not in docs\n",
    "- Users get outdated or incorrect information\n",
    "- Takes 5-10 minutes to manually search docs\n",
    "- Support team wastes time answering FAQ questions\n",
    "\n",
    "**The Solution (RAG):**\n",
    "1. **Embed** all documentation into a vector database\n",
    "2. **Retrieve** the most relevant docs based on user question\n",
    "3. **Generate** an answer grounded in those specific documents\n",
    "4. **Cite sources** so users can verify information\n",
    "\n",
    "**Business Impact:**\n",
    "- ‚è±Ô∏è Answer docs questions in <5 seconds vs 5-10 minutes manual search\n",
    "- ‚úÖ Zero hallucinations (answers grounded in real docs)\n",
    "- üí∞ Save $80K/year in support time\n",
    "- üòä Improve customer satisfaction (consistent, accurate answers)\n",
    "- üöÄ Scale support without hiring\n",
    "\n",
    "Companies like **GitHub Copilot Docs, Notion Help AI, Intercom** use RAG in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9604a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''\n",
    "<style>\n",
    ".jp-RenderedHTMLCommon h2 {\n",
    "    color: #2c3e50;\n",
    "    border-bottom: 2px solid #3498db;\n",
    "    padding-bottom: 10px;\n",
    "    margin-top: 30px;\n",
    "}\n",
    ".jp-RenderedHTMLCommon h3 {\n",
    "    color: #34495e;\n",
    "    margin-top: 20px;\n",
    "}\n",
    ".exercise-box {\n",
    "    background-color: #fff3cd;\n",
    "    border-left: 5px solid #ffc107;\n",
    "    padding: 15px;\n",
    "    margin: 20px 0;\n",
    "    border-radius: 5px;\n",
    "}\n",
    ".scenario-box {\n",
    "    background-color: #d1ecf1;\n",
    "    border-left: 5px solid #17a2b8;\n",
    "    padding: 15px;\n",
    "    margin: 20px 0;\n",
    "    border-radius: 5px;\n",
    "}\n",
    ".rag-box {\n",
    "    background-color: #e8f5e9;\n",
    "    border-left: 5px solid #4caf50;\n",
    "    padding: 15px;\n",
    "    margin: 20px 0;\n",
    "    border-radius: 5px;\n",
    "}\n",
    "</style>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758d85b",
   "metadata": {},
   "source": [
    "## üîç Part 1: Understanding Embeddings\n",
    "\n",
    "### What Are Embeddings?\n",
    "\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning.\n",
    "\n",
    "**Visual Example:**\n",
    "\n",
    "```\n",
    "Text: \"How do I reset my password?\"\n",
    "      ‚Üì\n",
    "Embedding: [-0.123, 0.456, -0.789, 0.234, ...] (1536 dimensions)\n",
    "\n",
    "Text: \"I forgot my password, how to recover?\"\n",
    "      ‚Üì\n",
    "Embedding: [-0.125, 0.458, -0.787, 0.235, ...] (very similar!)\n",
    "```\n",
    "\n",
    "**Key Properties:**\n",
    "- **Semantic similarity:** Similar texts have similar embeddings\n",
    "- **Fixed dimensions:** Always same size (e.g., 1536 dims for OpenAI)\n",
    "- **Dense vectors:** All numbers matter (unlike sparse one-hot encoding)\n",
    "- **Normalized:** Can use similarity metrics (cosine distance)\n",
    "\n",
    "**Why This Matters:**\n",
    "- \"Reset password\" and \"Recover account\" are semantically similar\n",
    "- But raw text comparison would miss this\n",
    "- Embeddings capture the semantic connection\n",
    "\n",
    "### Embedding Space Visualization\n",
    "\n",
    "```\n",
    "2D Projection of Real Embeddings:\n",
    "\n",
    "            \"Learn Python\"\n",
    "                   |\n",
    "          \"Programming tutorial\"\n",
    "                 /|\\\n",
    "                / | \\\n",
    "    \"Java guide\" | \"Python basics\"  ‚Üí All clustered together!\n",
    "                \\ | /               These are semantically similar\n",
    "          \"Coding lesson\"\n",
    "                   |\n",
    "            \"Write code\"\n",
    "\n",
    "                   PROGRAMMING CLUSTER\n",
    "\n",
    "------- SEMANTIC SPACE DIVIDER -------\n",
    "\n",
    "           \"Check balance\"\n",
    "                   |\n",
    "          \"Account balance query\"\n",
    "                 /|\\\n",
    "                / | \\\n",
    "  \"View account\" | \"Money status\"   ‚Üí Different cluster!\n",
    "                \\ | /               Banking/Financial semantic\n",
    "          \"How much do I have?\"\n",
    "                   |\n",
    "            \"Account status\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b67fc",
   "metadata": {},
   "source": [
    "## üìö Part 2: Building RAG Systems\n",
    "\n",
    "### RAG Pipeline Architecture\n",
    "\n",
    "```\n",
    "RAG SYSTEM FLOW:\n",
    "\n",
    "OFFLINE (Setup once):\n",
    "  1. Documents ‚Üí Load & Split ‚Üí [\"chunk1\", \"chunk2\", ...]\n",
    "  2. Embed Chunks ‚Üí [vector1, vector2, ...]\n",
    "  3. Store in Vector DB ‚Üí indexed and queryable\n",
    "\n",
    "ONLINE (For each user query):\n",
    "  1. User Question ‚Üí Embed the question\n",
    "  2. Semantic Search ‚Üí Find K nearest documents\n",
    "  3. Build Context ‚Üí \"Based on these docs: ...\"\n",
    "  4. Generate Answer ‚Üí LLM answers using context\n",
    "  5. Return Result ‚Üí \"Here's your answer (sources: docs 1, 3, 5)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a1242",
   "metadata": {},
   "source": [
    "### Document Chunking Strategies\n",
    "\n",
    "**Why chunking matters:**\n",
    "- Can't embed entire 500-page document (too long, loses relevance)\n",
    "- Need to split into meaningful segments\n",
    "- Optimal chunk size: 500-2000 characters\n",
    "\n",
    "**Chunking Strategies:**\n",
    "\n",
    "1. **Fixed Size:** 1000 char chunks\n",
    "   - ‚úÖ Simple, predictable\n",
    "   - ‚ùå Might split sentences\n",
    "\n",
    "2. **Semantic:** Split at section boundaries\n",
    "   - ‚úÖ Preserves meaning\n",
    "   - ‚ùå Variable sizes\n",
    "\n",
    "3. **Overlap:** Chunks overlap by 20%\n",
    "   - ‚úÖ Preserves context at boundaries\n",
    "   - ‚ùå Slightly redundant storage\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "DOCUMENT:\n",
    "\"How to reset your password. Step 1: Click Settings. Step 2: Click Security. Step 3: Click Change Password. Enter your new password twice.\"\n",
    "\n",
    "CHUNKED (overlap=20%):\n",
    "Chunk 1: \"How to reset your password. Step 1: Click Settings. Step 2: Click Security.\"\n",
    "Chunk 2: \"Step 2: Click Security. Step 3: Click Change Password. Enter your new password twice.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec96e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Building a Simple RAG System\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "# Sample documentation\n",
    "DOCUMENTATION = {\n",
    "    \"api-auth\": \"\"\"# API Authentication\n",
    "\n",
    "## Overview\n",
    "All API requests require authentication using an API key.\n",
    "\n",
    "## Getting Your API Key\n",
    "1. Log in to your account\n",
    "2. Go to Settings > API Keys\n",
    "3. Click 'Generate New Key'\n",
    "4. Copy the key (it won't be shown again!)\n",
    "\n",
    "## Using the API Key\n",
    "Include the key in the Authorization header:\n",
    "```\n",
    "curl -H \"Authorization: Bearer YOUR_API_KEY\" https://api.example.com/v1/users\n",
    "```\n",
    "\n",
    "## Key Rotation\n",
    "For security, rotate your API keys every 90 days.\n",
    "Old keys continue to work for 7 days after rotation.\"\"\",\n",
    "\n",
    "    \"password-reset\": \"\"\"# Password Reset Guide\n",
    "\n",
    "## Forgot Your Password?\n",
    "1. Click 'Forgot Password' on the login page\n",
    "2. Enter your email address\n",
    "3. Check your email for a reset link\n",
    "4. Click the link and create a new password\n",
    "5. Password must be at least 12 characters\n",
    "\n",
    "## Locked Account\n",
    "If you fail login 5 times in 30 minutes, your account is locked for 1 hour.\n",
    "\n",
    "## Resetting Through Admin\n",
    "Admins can reset user passwords from Settings > User Management.\"\"\",\n",
    "\n",
    "    \"billing-plans\": \"\"\"# Billing & Plans\n",
    "\n",
    "## Available Plans\n",
    "- Free: $0/month, up to 100 requests/day\n",
    "- Pro: $99/month, up to 100K requests/day\n",
    "- Enterprise: Custom pricing, unlimited requests\n",
    "\n",
    "## Billing Period\n",
    "- Monthly plans renew on the 1st of each month\n",
    "- Annual plans get 20% discount\n",
    "- Cancel anytime, no questions asked\n",
    "\n",
    "## Invoices\n",
    "Invoices are available in Settings > Billing.\n",
    "Download or email invoices for accounting.\"\"\"\n",
    "}\n",
    "\n",
    "# Step 1: Load and chunk documents\n",
    "print(\"üìÑ Step 1: Loading and Chunking Documents\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "documents = []\n",
    "for doc_name, content in DOCUMENTATION.items():\n",
    "    chunks = text_splitter.split_text(content)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"source\": doc_name,\n",
    "                \"chunk_index\": i\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "print(f\"Total documents loaded: {len(DOCUMENTATION)}\")\n",
    "print(f\"Total chunks created: {len(documents)}\")\n",
    "print()\n",
    "\n",
    "# Display sample chunks\n",
    "print(\"Sample Chunks:\")\n",
    "for i, doc in enumerate(documents[:3]):\n",
    "    print(f\"\\nChunk {i+1} (from {doc.metadata['source']}):\")\n",
    "    print(f\"  {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a29fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Simulated Embeddings and Vector Store\n",
    "\n",
    "print(\"\\nüî¢ Step 2: Creating Embeddings\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"Simple in-memory vector store for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, documents: List[Document]):\n",
    "        self.documents = documents\n",
    "        # In real scenarios, use OpenAI embeddings, Sentence Transformers, etc.\n",
    "        self.embeddings = self._create_fake_embeddings()\n",
    "    \n",
    "    def _create_fake_embeddings(self):\n",
    "        \"\"\"Create fake embeddings for demo (hash-based, not semantic).\"\"\"\n",
    "        embeddings = []\n",
    "        for doc in self.documents:\n",
    "            # Simple hash-based \"embedding\" for demo\n",
    "            np.random.seed(hash(doc.page_content) % 2**32)\n",
    "            embedding = np.random.randn(768)  # 768-dimensional\n",
    "            embedding = embedding / np.linalg.norm(embedding)  # Normalize\n",
    "            embeddings.append(embedding)\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def similarity_search(self, query: str, k: int = 3):\n",
    "        \"\"\"Find k most similar documents to query.\"\"\"\n",
    "        # Create query embedding\n",
    "        np.random.seed(hash(query) % 2**32)\n",
    "        query_embedding = np.random.randn(768)\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        scores = np.dot(self.embeddings, query_embedding)\n",
    "        \n",
    "        # Get top k\n",
    "        top_indices = np.argsort(scores)[::-1][:k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'document': self.documents[idx],\n",
    "                'score': float(scores[idx])\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create vector store\n",
    "vector_store = SimpleVectorStore(documents)\n",
    "print(f\"Vector store created with {len(documents)} documents\")\n",
    "print(f\"Each embedding: 768 dimensions\")\n",
    "print(f\"Total storage: {len(documents) * 768 * 4 / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065cf726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query the Vector Store\n",
    "\n",
    "print(\"\\nüîç Step 3: Testing RAG Queries\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_queries = [\n",
    "    \"How do I get my API key?\",\n",
    "    \"I forgot my password, what should I do?\",\n",
    "    \"What's the price of the Pro plan?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n‚ùì Query: {query}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    results = vector_store.similarity_search(query, k=2)\n",
    "    \n",
    "    print(\"Retrieved Documents:\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        doc = result['document']\n",
    "        score = result['score']\n",
    "        print(f\"\\n  [{i}] Source: {doc.metadata['source']} (similarity: {score:.3f})\")\n",
    "        print(f\"      {doc.page_content[:150]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ RAG retrieval working! These documents would be passed to the LLM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abff5fa6",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Part 4: Complete RAG System\n",
    "\n",
    "<div class=\"rag-box\">\n",
    "<strong>Building End-to-End RAG:</strong><br><br>\n",
    "Combine retrieval with generation for complete answers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4557cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.llms.fake import FakeListLLM\n",
    "\n",
    "# RAG Prompt - instructs LLM to use retrieved docs\n",
    "rag_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful customer support assistant. \n",
    "\n",
    "Using ONLY the following documentation, answer the user's question.\n",
    "If the answer is not in the documentation, say \"I don't have that information.\"\n",
    "\n",
    "DOCUMENTATION:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    ")\n",
    "\n",
    "# Simulated LLM responses\n",
    "llm = FakeListLLM(responses=[\n",
    "    \"To get your API key: 1) Log in to your account, 2) Go to Settings > API Keys, 3) Click 'Generate New Key', 4) Copy the key (it won't be shown again). Remember to rotate your key every 90 days for security.\",\n",
    "    \"If you forgot your password: 1) Click 'Forgot Password' on the login page, 2) Enter your email, 3) Check your email for a reset link, 4) Click the link and create a new password. Your new password must be at least 12 characters long.\",\n",
    "    \"Our Pro plan costs $99/month and includes up to 100K requests per day. We also offer a Free plan ($0/month with 100 requests/day) and an Enterprise plan with custom pricing and unlimited requests.\"\n",
    "])\n",
    "\n",
    "def rag_query(query: str):\n",
    "    \"\"\"Execute a RAG query.\"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=3)\n",
    "    \n",
    "    # Step 2: Format context from retrieved docs\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"[From {doc['document'].metadata['source']}]\\n{doc['document'].page_content}\"\n",
    "        for doc in retrieved_docs\n",
    "    ])\n",
    "    \n",
    "    # Step 3: Generate answer using LLM\n",
    "    answer = llm.predict(\n",
    "        context=context,\n",
    "        question=query\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'sources': [doc['document'].metadata['source'] for doc in retrieved_docs],\n",
    "        'retrieved_docs': retrieved_docs\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ RAG System Ready!\")\n",
    "print(\"\\nTesting RAG queries...\\n\")\n",
    "\n",
    "# Test the RAG system\n",
    "test_queries = [\n",
    "    \"How do I get my API key?\",\n",
    "    \"I forgot my password\",\n",
    "    \"How much is the Pro plan?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    result = rag_query(query)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚ùì Question: {result['query']}\")\n",
    "    print(f\"\\nüí¨ Answer:\")\n",
    "    print(f\"   {result['answer']}\")\n",
    "    print(f\"\\nüìö Sources: {', '.join(result['sources'])}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ RAG System Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddfda97",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Hands-On Exercises\n",
    "\n",
    "<div class=\"exercise-box\">\n",
    "<strong>üéØ Exercise 1: Document Chunking Optimization</strong><br><br>\n",
    "Experiment with different chunking strategies:\n",
    "<ol>\n",
    "<li>Fixed size chunks (500, 1000, 2000 chars)</li>\n",
    "<li>Chunks with different overlap (0%, 20%, 50%)</li>\n",
    "<li>Semantic chunking at section boundaries</li>\n",
    "</ol>\n",
    "<br>\n",
    "<strong>Measure:</strong> Which strategy gives best retrieval accuracy?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ef002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation here!\n",
    "# Test different chunk sizes and overlaps\n",
    "# Measure retrieval quality\n",
    "\n",
    "print(\"Your chunking optimization here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045c2101",
   "metadata": {},
   "source": [
    "<div class=\"exercise-box\">\n",
    "<strong>üéØ Exercise 2: Build a Documentation Assistant</strong><br><br>\n",
    "Create a RAG system for your own documentation:\n",
    "<ol>\n",
    "<li>Load your own documents (markdown, PDF, or text files)</li>\n",
    "<li>Chunk them intelligently</li>\n",
    "<li>Create embeddings</li>\n",
    "<li>Test with 5+ queries</li>\n",
    "<li>Measure answer accuracy</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748cd0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation here!\n",
    "print(\"Your documentation assistant here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190fd97",
   "metadata": {},
   "source": [
    "<div class=\"exercise-box\">\n",
    "<strong>üéØ Exercise 3: RAG with Hybrid Search</strong><br><br>\n",
    "Improve retrieval with hybrid search:\n",
    "<ol>\n",
    "<li>Semantic search (vector similarity)</li>\n",
    "<li>Keyword search (BM25)</li>\n",
    "<li>Combine both for better results</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b064baef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation here!\n",
    "print(\"Your hybrid search implementation here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a2e4e5",
   "metadata": {},
   "source": [
    "## ü§î Reflection Questions\n",
    "\n",
    "**Q1: Why is RAG better than fine-tuning for knowledge updates?**\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "<strong>RAG advantages:</strong>\n",
    "<ul>\n",
    "<li>Update docs instantly (no retraining)</li>\n",
    "<li>Always cite sources (transparency)</li>\n",
    "<li>Change sources without changing model</li>\n",
    "<li>Much cheaper than fine-tuning</li>\n",
    "<li>Easier to debug (see which docs were retrieved)</li>\n",
    "</ul>\n",
    "<strong>Fine-tuning advantages:</strong>\n",
    "<ul>\n",
    "<li>Knowledge deeply integrated</li>\n",
    "<li>Faster inference (no retrieval step)</li>\n",
    "<li>Smaller model footprint</li>\n",
    "</ul>\n",
    "<strong>Best practice:</strong> Use RAG for frequently-updated knowledge, fine-tune for core domain knowledge.\n",
    "</details>\n",
    "\n",
    "**Q2: How do you measure RAG quality?**\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "<ol>\n",
    "<li><strong>Retrieval metrics:</strong> Did we retrieve relevant documents? (Precision@K, Recall)</li>\n",
    "<li><strong>Generation metrics:</strong> Is the answer good? (BLEU, ROUGE, semantic similarity)</li>\n",
    "<li><strong>Human evaluation:</strong> Is it actually helpful? (manual rating 1-5)</li>\n",
    "<li><strong>Hallucination rate:</strong> How often does LLM invent facts?</li>\n",
    "<li><strong>Latency:</strong> Is it fast enough? (<500ms for retrieval + generation)</li>\n",
    "</ol>\n",
    "</details>\n",
    "\n",
    "**Q3: When should you use different embedding models?**\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "<ul>\n",
    "<li><strong>OpenAI text-embedding-3-large:</strong> Best quality, $0.13 per 1M tokens</li>\n",
    "<li><strong>Cohere Embed:</strong> Fast, cost-effective</li>\n",
    "<li><strong>Sentence Transformers:</strong> Free, open-source, run locally</li>\n",
    "<li><strong>Domain-specific:</strong> Medical docs need medical embeddings (e.g., BioBERT)</li>\n",
    "</ul>\n",
    "<strong>Rule of thumb:</strong> Start with text-embedding-3-small (fast, cheap), upgrade if accuracy is insufficient.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25680f99",
   "metadata": {},
   "source": [
    "## üìù Week 4 Project: Knowledge Base Assistant\n",
    "\n",
    "**Build a complete RAG-powered knowledge base assistant.**\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "**Functionality:**\n",
    "1. Load documentation (at least 5 documents, 5+ KB each)\n",
    "2. Split intelligently (experiment with chunk sizes)\n",
    "3. Create embeddings and vector store\n",
    "4. Retrieve relevant documents for queries\n",
    "5. Generate grounded answers with citations\n",
    "\n",
    "**Quality Metrics:**\n",
    "- Test with 20+ queries\n",
    "- Measure retrieval accuracy (did we get relevant docs?)\n",
    "- Human evaluation (are answers helpful?)\n",
    "- Track hallucinations (does LLM invent facts?)\n",
    "\n",
    "**Advanced Features (bonus):**\n",
    "- Hybrid search (semantic + keyword)\n",
    "- Re-ranking retrieved docs\n",
    "- Query expansion (expand query before search)\n",
    "- Chat history awareness\n",
    "\n",
    "### Deliverables:\n",
    "- Document loading and chunking pipeline\n",
    "- Embedding creation and storage\n",
    "- Similarity search implementation\n",
    "- RAG generation with citations\n",
    "- Quality analysis and metrics\n",
    "- User interface or API endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50913cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 4 Project Starter\n",
    "\n",
    "# TODO: Load your documents\n",
    "# TODO: Implement chunking with configurable chunk_size and overlap\n",
    "# TODO: Create embeddings and vector store\n",
    "# TODO: Build RAG query function\n",
    "# TODO: Test with 20+ queries\n",
    "# TODO: Evaluate quality (retrieval accuracy, hallucination rate)\n",
    "\n",
    "print(\"üéØ Your knowledge base assistant implementation here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64548624",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "**What you learned this week:**\n",
    "\n",
    "‚úÖ **Embeddings:**\n",
    "- Convert text to semantic vectors\n",
    "- Similar texts have similar embeddings\n",
    "- Enable semantic search in vector stores\n",
    "\n",
    "‚úÖ **Document Processing:**\n",
    "- Chunking strategies and trade-offs\n",
    "- Overlap importance for context\n",
    "- Metadata preservation\n",
    "\n",
    "‚úÖ **RAG Architecture:**\n",
    "- Retrieve relevant docs based on query\n",
    "- Use retrieved docs as context\n",
    "- Generate grounded, cited answers\n",
    "\n",
    "‚úÖ **Real-world applications:**\n",
    "- Documentation assistants\n",
    "- Customer support automation\n",
    "- Knowledge base Q&A\n",
    "- Internal wiki search\n",
    "\n",
    "## üîú Next Week: Evaluation & Debugging\n",
    "\n",
    "In Week 5, we'll master quality assurance:\n",
    "- Measuring LLM output quality\n",
    "- Debugging failures\n",
    "- A/B testing different approaches\n",
    "- Production monitoring\n",
    "\n",
    "**Preview question:** How would you detect and fix hallucinations in your RAG system?\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "- [LangChain RAG Documentation](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "- [Vector Store Comparison](https://python.langchain.com/docs/integrations/vectorstores/)\n",
    "- [Embeddings Guide](https://python.langchain.com/docs/integrations/text_embedding/)\n",
    "- [RAG Research Papers](https://arxiv.org/abs/2005.11401)\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations on completing Week 4!** You can now build production RAG systems that ground LLMs in real data. See you next week! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
