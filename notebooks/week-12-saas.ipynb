{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 12 \u2014 Capstone: End-to-End ML Pipeline\n\n**Course:** Applied ML Foundations for SaaS Analytics  \n**Week Focus:** Build complete ML system from data to deployment.\n\n---\n\n## \ud83c\udfaf Learning Objectives\n\n- Implement full ML pipeline (ETL \u2192 Features \u2192 Training \u2192 Evaluation \u2192 Deployment)\n- Production-ready error handling and monitoring\n- Model versioning and reproducibility\n- Performance monitoring in production\n- Real-world considerations (data drift, retraining)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd, numpy as np\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve, confusion_matrix\nfrom datetime import datetime\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(f\"Pipeline started at {datetime.now().isoformat()}\")\n\n# Load all data sources\nsubs = pd.read_csv('../data/subscriptions.csv', parse_dates=['signup_date', 'churn_date'])\nfeature_usage = pd.read_csv('../data/feature_usage.csv')\nuser_events = pd.read_csv('../data/user_events.csv')\n\nprint(f\"\u2713 Data loaded: {len(subs)} subscriptions, {len(feature_usage)} feature usage events\")\nprint(f\"  {len(user_events)} user events\")\nprint(f\"  Churn rate: {(subs['churn_date'].notna().sum() / len(subs) * 100):.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Feature Engineering Pipeline\n\n**Key principle:** Create features from multiple data sources\n\n**Challenges:**\n- Handle missing values\n- Normalize/scale features\n- Feature selection\n- Feature versioning\n\n**\ud83d\udca1 Depth Note:** How does feature engineering impact model performance? A/B test feature sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering\nengagement = feature_usage.groupby('user_id').agg({\n    'usage_count': 'sum',\n    'feature_name': 'nunique',\n    'last_used': lambda x: (datetime.now() - pd.to_datetime(x.max())).days if pd.notna(x.max()) else np.nan\n}).reset_index()\nengagement.columns = ['user_id', 'total_usage', 'features_adopted', 'days_since_active']\n\nevents = user_events.groupby('user_id').size().reset_index(name='total_events')\n\ndf = subs[['user_id', 'signup_date', 'tenure_days', 'mrr', 'plan_tier', 'churn_date']].merge(engagement, on='user_id', how='left')\ndf = df.merge(events, on='user_id', how='left').fillna(0)\ndf['churned'] = df['churn_date'].notna().astype(int)\n\n# Time-based split (critical for production!)\ndf['signup_month'] = pd.to_datetime(df['signup_date']).dt.to_period('M')\nlatest_month = df['signup_month'].max()\ntrain_cutoff = latest_month - 1  # Leave last month for testing\n\ndf_train = df[df['signup_month'] <= train_cutoff]\ndf_test = df[df['signup_month'] > train_cutoff]\n\nprint(f\"\\nTime-based split:\")\nprint(f\"  Train: {len(df_train)} customers (months up to {train_cutoff})\")\nprint(f\"  Test:  {len(df_test)} customers (month {latest_month})\")\nprint(f\"  \u2713 Prevents data leakage from future data\")\n\nfeatures = ['tenure_days', 'mrr', 'total_usage', 'features_adopted', 'total_events', 'days_since_active']\nX_train = df_train[features]\ny_train = df_train['churned']\nX_test = df_test[features]\ny_test = df_test['churned']\n\nprint(f\"\\nFeatures: {features}\")\nprint(f\"  Train churn: {y_train.sum()} / {len(y_train)} ({y_train.mean()*100:.1f}%)\")\nprint(f\"  Test churn:  {y_test.sum()} / {len(y_test)} ({y_test.mean()*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Model Training with Cross-Validation\n\n**Why cross-validation?** More robust estimate of performance\n\n**Stratified K-Fold:** Maintain class distribution in each fold\n\n**\ud83d\udca1 Depth Note:** How does CV affect model selection? What if you don't have enough data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Cross-validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nmodel = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n\n# CV scores\ncv_scores = cross_val_score(model, X_train_scaled, y_train, cv=skf, scoring='roc_auc')\n\nprint(f\"Cross-validation results (5-fold):\")\nfor i, score in enumerate(cv_scores):\n    print(f\"  Fold {i+1}: {score:.4f}\")\nprint(f\"\\n  Mean: {cv_scores.mean():.4f}\")\nprint(f\"  Std:  {cv_scores.std():.4f}\")\nprint(f\"  95% CI: [{cv_scores.mean() - 1.96*cv_scores.std():.4f}, {cv_scores.mean() + 1.96*cv_scores.std():.4f}]\")\n\n# Train final model\nmodel.fit(X_train_scaled, y_train)\nprint(f\"\\n\u2713 Model trained on {len(X_train_scaled)} training examples\")\n\n# Test performance\ny_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\nauc = roc_auc_score(y_test, y_pred_proba)\n\nprint(f\"\\nTest set AUC: {auc:.4f}\")\nprint(f\"Generalization: {cv_scores.mean():.4f} (CV) \u2192 {auc:.4f} (Test)\")\n\nif abs(cv_scores.mean() - auc) > 0.05:\n    print(f\"\u26a0\ufe0f  Warning: >5% gap suggests possible data distribution shift\")\nelse:\n    print(f\"\u2713 Good generalization - CV and test performance align\")\n\ntp, fp, th = precision_recall_curve(y_test, y_pred_proba)[:3]\nthresholds = np.linspace(0, 1, 11)\nfor thresh in [0.3, 0.5, 0.7]:\n    y_pred = (y_pred_proba >= thresh).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    print(f\"\\nThreshold {thresh}: Precision={precision:.2%}, Recall={recall:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Production Deployment Checklist\n\n**Before shipping:**\n- Model versioning \u2713\n- Prediction latency < 100ms \u2713\n- Handle missing values \u2713\n- Input validation \u2713\n- Error monitoring \u2713\n- Feature drift detection \u2713\n\n**\ud83d\udca1 Depth Note:** How do you monitor model performance in production? What triggers retraining?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model and scaler for production\nversion = datetime.now().strftime('%Y%m%d_%H%M%S')\nmodel_path = f'churn_model_{version}.pkl'\nscaler_path = f'scaler_{version}.pkl'\n\nwith open(model_path, 'wb') as f:\n    pickle.dump(model, f)\nwith open(scaler_path, 'wb') as f:\n    pickle.dump(scaler, f)\n\nprint(f\"Model saved: {model_path}\")\nprint(f\"Scaler saved: {scaler_path}\")\n\nprint(f\"\\n\" + \"=\"*60)\nprint(f\"PRODUCTION DEPLOYMENT SUMMARY\")\nprint(f\"=\"*60)\nprint(f\"\\nModel Metrics:\")\nprint(f\"  AUC-ROC: {auc:.4f}\")\nprint(f\"  Training examples: {len(X_train):,}\")\nprint(f\"  Features: {len(features)}\")\nprint(f\"  Model type: {type(model).__name__}\")\nprint(f\"\\nDeployment Info:\")\nprint(f\"  Version: {version}\")\nprint(f\"  Timestamp: {datetime.now().isoformat()}\")\nprint(f\"  Ready for production: \u2713\")\nprint(f\"\\nMonitoring Checklist:\")\nprint(f\"  [ ] Set up performance monitoring\")\nprint(f\"  [ ] Track feature distributions\")\nprint(f\"  [ ] Alert on prediction drift\")\nprint(f\"  [ ] Schedule weekly retraining\")\nprint(f\"  [ ] Document feature definitions\")\nprint(f\"  [ ] Test failure scenarios\")\nprint(f\"  [ ] Set SLA targets (latency, availability)\")\nprint(f\"\\n\ud83d\ude80 Pipeline complete. Ready for deployment!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n\n\u2705 End-to-end ML pipeline architecture  \n\u2705 Time-based validation prevents data leakage  \n\u2705 Cross-validation for robust performance estimates  \n\u2705 Production readiness checklist  \n\u2705 Model versioning and monitoring  \n\u2705 Handling real-world challenges (drift, retraining)  \n\n## \ud83c\udf93 Congratulations!\n\nYou've completed the Applied ML Foundations course. You now understand:\n- Data preparation and EDA\n- Supervised learning (regression, classification)\n- Unsupervised learning (clustering, dimensionality reduction)\n- Ensemble methods\n- Deep learning fundamentals\n- Production ML systems\n\n**Next steps:**\n1. Apply to your own datasets\n2. Dive deeper into specialized areas\n3. Learn MLOps (deployment, monitoring)\n4. Explore advanced techniques (NLP, Computer Vision)\n\n**Resources:**\n- Scikit-learn documentation\n- TensorFlow/Keras tutorials\n- Fast.ai practical deep learning\n- Real-world ML (testing, debugging, infrastructure)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}