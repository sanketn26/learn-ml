{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 â€” Unsupervised Learning: Clustering\n",
    "\n",
    "**Course:** Applied ML Foundations for SaaS Analytics  \n",
    "**Week Focus:** Discover natural customer segments without predefined labels.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "- Supervised vs Unsupervised Learning\n",
    "- K-Means clustering and optimal K selection\n",
    "- Cluster profiling and business interpretation\n",
    "- Compare K-Means, DBSCAN, Hierarchical approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Load data\n",
    "subs = pd.read_csv('../data/subscriptions.csv', parse_dates=['signup_date', 'churn_date'])\n",
    "feature_usage = pd.read_csv('../data/feature_usage.csv')\n",
    "user_events = pd.read_csv('../data/user_events.csv')\n",
    "\n",
    "print(f\"Customers: {len(subs):,} | Features: {len(feature_usage):,} | Events: {len(user_events):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Feature Engineering for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate engagement metrics\n",
    "engagement = feature_usage.groupby('user_id').agg({\n",
    "    'usage_count': 'sum',\n",
    "    'feature_name': 'nunique'\n",
    "}).rename(columns={'usage_count': 'total_usage', 'feature_name': 'features_adopted'}).reset_index()\n",
    "\n",
    "events = user_events.groupby('user_id').size().reset_index(name='total_events')\n",
    "\n",
    "# Merge features\n",
    "df = subs[['user_id', 'tenure_days', 'mrr', 'plan_tier']].merge(engagement, on='user_id', how='left')\n",
    "df = df.merge(events, on='user_id', how='left')\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Create derived features\n",
    "df['usage_per_day'] = df['total_usage'] / (df['tenure_days'] + 1)\n",
    "df['events_per_day'] = df['total_events'] / (df['tenure_days'] + 1)\n",
    "\n",
    "clustering_features = ['tenure_days', 'mrr', 'total_usage', 'features_adopted', 'total_events', 'usage_per_day', 'events_per_day']\n",
    "print(f\"Features: {clustering_features}\")\n",
    "print(f\"\\nFeature summary:\\n{df[clustering_features].describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature Scaling & K-Means Clustering\n",
    "\n",
    "**ðŸ’¡ Depth Note:** Feature scaling is CRITICAL for distance-based algorithms. Without scaling, features with larger ranges dominate. StandardScaler makes all features have mean=0, std=1.\n",
    "\n",
    "**Explore further:** Try MinMaxScaler vs StandardScaler â€” does output change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X = df[clustering_features].copy()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"âœ… Features scaled to mean=0, std=1\")\n",
    "\n",
    "# Find optimal K using Elbow Method + Silhouette Score\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINDING OPTIMAL K\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    silhouette = silhouette_score(X_scaled, kmeans.labels_)\n",
    "    results.append({'K': k, 'Inertia': kmeans.inertia_, 'Silhouette': silhouette})\n",
    "    print(f\"K={k} | Inertia: {kmeans.inertia_:>10,.0f} | Silhouette: {silhouette:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "best_k = results_df.loc[results_df['Silhouette'].idxmax(), 'K']\n",
    "print(f\"\\nâœ… Recommended K: {best_k} (silhouette: {results_df['Silhouette'].max():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train Final K-Means & Profile Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final K-Means model\n",
    "kmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "df['cluster'] = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"Cluster distribution:\")\n",
    "print(df['cluster'].value_counts().sort_index())\n",
    "\n",
    "# Profile each cluster\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLUSTER PROFILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "profile = df.groupby('cluster')[clustering_features + ['plan_tier']].agg({\n",
    "    'tenure_days': 'mean',\n",
    "    'mrr': 'mean',\n",
    "    'total_usage': 'mean',\n",
    "    'features_adopted': 'mean',\n",
    "    'total_events': 'mean',\n",
    "    'usage_per_day': 'mean',\n",
    "    'events_per_day': 'mean',\n",
    "    'plan_tier': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'Unknown'\n",
    "}).round(2)\n",
    "\n",
    "print(profile)\n",
    "\n",
    "# Assign cluster names based on characteristics\n",
    "cluster_names = {}\n",
    "for cluster_id in range(best_k):\n",
    "    c_profile = df[df['cluster'] == cluster_id]\n",
    "    avg_mrr = c_profile['mrr'].mean()\n",
    "    avg_usage = c_profile['usage_per_day'].mean()\n",
    "    \n",
    "    if avg_mrr > df['mrr'].quantile(0.75) and avg_usage > df['usage_per_day'].quantile(0.75):\n",
    "        cluster_names[cluster_id] = \"Power Users\"\n",
    "    elif avg_mrr > df['mrr'].quantile(0.75):\n",
    "        cluster_names[cluster_id] = \"High Payers\"\n",
    "    elif avg_usage > df['usage_per_day'].quantile(0.75):\n",
    "        cluster_names[cluster_id] = \"Active Users\"\n",
    "    elif c_profile['tenure_days'].mean() < df['tenure_days'].quantile(0.25):\n",
    "        cluster_names[cluster_id] = \"New Customers\"\n",
    "    elif avg_mrr < df['mrr'].quantile(0.25):\n",
    "        cluster_names[cluster_id] = \"Low Value\"\n",
    "    else:\n",
    "        cluster_names[cluster_id] = \"Steady Users\"\n",
    "\n",
    "df['segment'] = df['cluster'].map(cluster_names)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SEGMENT NAMES & BUSINESS INTERPRETATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for cluster_id in sorted(df['cluster'].unique()):\n",
    "    segment = cluster_names[cluster_id]\n",
    "    segment_size = (df['cluster'] == cluster_id).sum()\n",
    "    avg_mrr = df[df['cluster'] == cluster_id]['mrr'].mean()\n",
    "    print(f\"\\n{segment} (n={segment_size:,}): ${avg_mrr:.0f} avg MRR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Business Actions by Segment\n",
    "\n",
    "**ðŸ’¡ Depth Note:** This is where clustering becomes valuable. Design segment-specific strategies for:\n",
    "- Marketing campaigns\n",
    "- Pricing/upsell opportunities\n",
    "- Churn prevention\n",
    "- Product roadmap prioritization\n",
    "\n",
    "**Explore further:** Build a churn model with 'segment' as a feature â€” does it improve predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment-specific metrics\n",
    "print(\"=\" * 60)\n",
    "print(\"SEGMENT CHARACTERISTICS & RECOMMENDED ACTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for segment in df['segment'].unique():\n",
    "    segment_df = df[df['segment'] == segment]\n",
    "    churned = segment_df['churn_date'].notna().sum()\n",
    "    churn_rate = 100 * churned / len(segment_df)\n",
    "    \n",
    "    print(f\"\\n{segment}:\")\n",
    "    print(f\"  Size: {len(segment_df):,} ({100*len(segment_df)/len(df):.1f}%)\")\n",
    "    print(f\"  Avg MRR: ${segment_df['mrr'].mean():.0f}\")\n",
    "    print(f\"  Avg Tenure: {segment_df['tenure_days'].mean():.0f} days\")\n",
    "    print(f\"  Churn Rate: {churn_rate:.1f}%\")\n",
    "    print(f\"  Avg Features Adopted: {segment_df['features_adopted'].mean():.1f}\")\n",
    "    \n",
    "    if 'Power User' in segment:\n",
    "        print(f\"  Action: Assign account managers, prevent churn\")\n",
    "    elif 'High Payer' in segment:\n",
    "        print(f\"  Action: Low engagement risk, increase onboarding\")\n",
    "    elif 'Active User' in segment:\n",
    "        print(f\"  Action: Upsell opportunity, feature adoption\")\n",
    "    elif 'New' in segment:\n",
    "        print(f\"  Action: Optimize onboarding, reduce time-to-value\")\n",
    "    elif 'Low Value' in segment:\n",
    "        print(f\"  Action: Re-engagement or downgrade to free tier\")\n",
    "    else:\n",
    "        print(f\"  Action: Standard nurture campaigns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Alternative Clustering Algorithms\n",
    "\n",
    "**ðŸ’¡ Depth Note:** Compare different algorithms on same data:\n",
    "- **DBSCAN**: Finds arbitrary-shaped clusters, identifies outliers\n",
    "- **Hierarchical**: Creates dendrogram, shows cluster relationships\n",
    "\n",
    "**Explore further:** \n",
    "- Plot dendrograms to visualize cluster hierarchy\n",
    "- Analyze DBSCAN outliers (noise points) â€” are they actual anomalies?\n",
    "- Compare runtime on larger datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
    "df['cluster_dbscan'] = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "n_clusters_dbscan = len(set(df['cluster_dbscan'])) - (1 if -1 in df['cluster_dbscan'].values else 0)\n",
    "n_outliers = (df['cluster_dbscan'] == -1).sum()\n",
    "\n",
    "print(f\"DBSCAN Results:\")\n",
    "print(f\"  Clusters: {n_clusters_dbscan}\")\n",
    "print(f\"  Outliers (noise): {n_outliers:,} ({100*n_outliers/len(df):.1f}%)\")\n",
    "\n",
    "# Hierarchical\n",
    "hierarchical = AgglomerativeClustering(n_clusters=best_k, linkage='ward')\n",
    "df['cluster_hierarchical'] = hierarchical.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"\\nHierarchical Results:\")\n",
    "print(f\"  Clusters: {best_k}\")\n",
    "print(f\"  Silhouette: {silhouette_score(X_scaled, df['cluster_hierarchical']):.4f}\")\n",
    "\n",
    "# Comparison\n",
    "comparison = pd.DataFrame({\n",
    "    'Algorithm': ['K-Means', 'DBSCAN', 'Hierarchical'],\n",
    "    'Silhouette': [\n",
    "        silhouette_score(X_scaled, df['cluster']),\n",
    "        silhouette_score(X_scaled[df['cluster_dbscan'] != -1], df.loc[df['cluster_dbscan'] != -1, 'cluster_dbscan']) if n_clusters_dbscan > 1 else 0,\n",
    "        silhouette_score(X_scaled, df['cluster_hierarchical'])\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALGORITHM COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On Exercises\n",
    "\n",
    "### Exercise 1: Feature Selection Impact\n",
    "Try clustering with different feature sets:\n",
    "1. Engagement only: `total_usage`, `features_adopted`, `total_events`\n",
    "2. Financial only: `mrr`, `tenure_days`\n",
    "3. All features (current)\n",
    "\n",
    "Compare silhouette scores and cluster interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test different feature sets\n",
    "# engagement_features = ['total_usage', 'features_adopted', 'total_events']\n",
    "# financial_features = ['mrr', 'tenure_days']\n",
    "# TODO: Train K-Means on each, compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Churn Analysis by Segment\n",
    "1. Calculate churn rate for each cluster\n",
    "2. Which segments have highest churn?\n",
    "3. Use segment as feature in churn prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze churn by segment\n",
    "# TODO: Build churn model with and without segment feature\n",
    "# TODO: Compare AUC scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Cohort Analysis\n",
    "Cluster customers separately by signup cohort (monthly). Do segment characteristics change over time? What does this tell us about our product?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract signup month from subs['signup_date']\n",
    "# TODO: Cluster within each cohort\n",
    "# TODO: Compare profiles across time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: Customer Persona Development\n",
    "\n",
    "**Deliverables:**\n",
    "1. Cluster analysis report (3-5 segments)\n",
    "2. Persona card for each segment (name, characteristics, pain points, use cases)\n",
    "3. Strategic recommendations (prioritization, product, marketing)\n",
    "\n",
    "**Bonus:** Build segment predictor for new customers on signup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "âœ… Supervised vs Unsupervised Learning distinction  \n",
    "âœ… Feature scaling importance for distance-based algorithms  \n",
    "âœ… Elbow method & silhouette analysis for optimal K  \n",
    "âœ… Cluster profiling & business interpretation  \n",
    "âœ… Algorithm comparison (K-Means, DBSCAN, Hierarchical)  \n",
    "âœ… Translating clusters into actionable business strategies  \n",
    "\n",
    "## ðŸ”œ Next Week: Dimensionality Reduction (PCA)\n",
    "\n",
    "Reduce hundreds of features â†’ 2-3 dimensions for visualization and noise reduction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
