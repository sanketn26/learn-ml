{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 5 ‚Äî Feature Engineering & Data Preprocessing\n",
        "\n",
        "**Course:** Applied ML Foundations for SaaS Analytics  \n",
        "**Week Focus:** Transform raw data into powerful features that capture customer behavior and business signals.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this week, you will:\n",
        "- Encode categorical variables (one-hot, target encoding)\n",
        "- Handle missing data strategically\n",
        "- Scale and normalize numerical features\n",
        "- Create interaction and polynomial features\n",
        "- Build domain-driven features from business knowledge\n",
        "- Detect and handle outliers appropriately\n",
        "- Validate feature quality and distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "style"
        ]
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "HTML('''\n",
        "<style>\n",
        "details {\n",
        "  margin: 10px 0;\n",
        "  padding: 8px 12px;\n",
        "  border: 1px solid #d9e2ec;\n",
        "  border-radius: 8px;\n",
        "  background: #f9fbfd;\n",
        "}\n",
        "details summary {\n",
        "  font-weight: 600;\n",
        "  color: #0056b3;\n",
        "  cursor: pointer;\n",
        "}\n",
        "details[open] {\n",
        "  background: #f1f7ff;\n",
        "  border-color: #c3d4f0;\n",
        "}\n",
        "details pre {\n",
        "  background: #f8f9fa;\n",
        "  padding: 8px;\n",
        "  border-radius: 6px;\n",
        "}\n",
        "</style>\n",
        "''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üè¢ Scenario ‚Äî Build a Churn Prediction Dataset\n",
        "\n",
        "You need to train a churn prediction model. Raw data has:\n",
        "- Mixed types: plan_tier (categorical), signup_date (datetime), usage_count (numeric)\n",
        "- Missing values: some users have no feature usage data\n",
        "- Outliers: a few power users with 1000x normal usage\n",
        "- Business knowledge: days_since_signup, recent_engagement_change, plan_change_count\n",
        "\n",
        "Task: Transform into a clean, ML-ready dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úçÔ∏è Hands-on Exercises\n",
        "\n",
        "1. **Categorical Encoding**: One-hot encode plan_tier, region, and customer_segment\n",
        "2. **Temporal Features**: From signup_date, create: days_active, months_active, signup_quarter, is_recent\n",
        "3. **Scaling**: Normalize usage_count and revenue features with StandardScaler or MinMaxScaler\n",
        "4. **Interactions**: Create plan_tier √ó region, recent_usage √ó lifetime features\n",
        "5. **Missing Data Strategy**: Define whether to drop, fill with mean, or use indicator variables for each column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>üí° Hint ‚Äî Feature Engineering Workflow</summary>\n",
        "\n",
        "**Step 1: Understand Data Types**\n",
        "```python\n",
        "df.dtypes  # what are we working with?\n",
        "df.isnull().sum()  # where are the gaps?\n",
        "```\n",
        "\n",
        "**Step 2: Missing Data Strategy**\n",
        "- Numeric: mean/median imputation or \"unknown\" indicator\n",
        "- Categorical: mode or \"Unknown\" category\n",
        "- When to drop: if > 50% missing in a feature\n",
        "\n",
        "**Step 3: Encode Categoricals**\n",
        "- Few categories (< 5): one-hot encoding\n",
        "- Many categories (> 100): target encoding or embedding\n",
        "\n",
        "**Step 4: Scale/Normalize**\n",
        "- Tree models: no scaling needed\n",
        "- Linear models, neural nets, KNN: use StandardScaler or MinMaxScaler\n",
        "\n",
        "**Step 5: Feature Selection**\n",
        "- Remove highly correlated features\n",
        "- Remove near-zero variance features\n",
        "- Use domain knowledge to keep business-meaningful features\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>‚úÖ Solution ‚Äî Complete Feature Engineering Pipeline</summary>\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from datetime import datetime\n",
        "\n",
        "# Load and merge data\n",
        "subs = pd.read_csv('../data/subscriptions.csv', parse_dates=['signup_date','churn_date'])\n",
        "feature_usage = pd.read_csv('../data/feature_usage.csv')\n",
        "\n",
        "# Aggregate features by user\n",
        "user_features = feature_usage.groupby('user_id').agg({\n",
        "    'usage_count': 'sum',\n",
        "    'feature_name': 'nunique'\n",
        "}).rename(columns={'usage_count': 'total_usage', 'feature_name': 'num_features'})\n",
        "\n",
        "# Merge\n",
        "df = subs.merge(user_features, left_on='user_id', right_index=True, how='left')\n",
        "\n",
        "# FEATURE ENGINEERING\n",
        "# 1. Create temporal features\n",
        "today = pd.Timestamp.now()\n",
        "df['days_active'] = (df['churn_date'].fillna(today) - df['signup_date']).dt.days\n",
        "df['signup_month'] = df['signup_date'].dt.month\n",
        "df['signup_quarter'] = df['signup_date'].dt.quarter\n",
        "\n",
        "# 2. Handle missing engagement data\n",
        "df['total_usage'] = df['total_usage'].fillna(0)\n",
        "df['num_features'] = df['num_features'].fillna(0)\n",
        "\n",
        "# 3. Create target: churned in next 30 days?\n",
        "df['target'] = df['churn_date'].notna().astype(int)\n",
        "\n",
        "# 4. Encode categorical\n",
        "df_encoded = pd.get_dummies(df, columns=['plan_tier'], drop_first=True)\n",
        "\n",
        "# 5. Scale numeric features\n",
        "scaler = StandardScaler()\n",
        "numeric_cols = ['days_active', 'total_usage', 'num_features']\n",
        "df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n",
        "\n",
        "# Result: ML-ready dataset\n",
        "print(f\"Feature matrix shape: {df_encoded.shape}\")\n",
        "print(f\"Null values: {df_encoded.isnull().sum().sum()}\")\n",
        "print(f\"Target distribution: {df_encoded['target'].value_counts().to_dict()}\")\n",
        "```\n",
        "\n",
        "**Why this works:**\n",
        "- Clear separation: raw data ‚Üí aggregation ‚Üí features ‚Üí encoding ‚Üí scaling\n",
        "- Handles missing values explicitly\n",
        "- Target clearly defined\n",
        "- Ready for train/test split and modeling\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"WEEK 5: FEATURE ENGINEERING DEMO\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Load data\n",
        "subs = pd.read_csv('../data/subscriptions.csv', parse_dates=['signup_date','churn_date'])\n",
        "feature_usage = pd.read_csv('../data/feature_usage.csv')\n",
        "\n",
        "print(\"\\n1. DATA PREPARATION\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Subscriptions: {len(subs)} records\")\n",
        "print(f\"Feature usage: {len(feature_usage)} records\")\n",
        "\n",
        "# Aggregate user metrics\n",
        "user_features = feature_usage.groupby('user_id').agg({\n",
        "    'usage_count': 'sum',\n",
        "    'feature_name': 'nunique'\n",
        "}).rename(columns={'usage_count': 'total_usage', 'feature_name': 'num_features'})\n",
        "\n",
        "print(f\"Unique users in feature data: {len(user_features)}\")\n",
        "\n",
        "# Merge\n",
        "df = subs.merge(user_features, left_on='user_id', right_index=True, how='left')\n",
        "print(f\"After merge: {len(df)} subscriptions with feature data\")\n",
        "\n",
        "print(\"\\n2. FEATURE ENGINEERING\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Temporal features\n",
        "from datetime import datetime\n",
        "today = pd.Timestamp.now()\n",
        "df['days_active'] = (df['churn_date'].fillna(today) - df['signup_date']).dt.days\n",
        "df['is_churned'] = df['churn_date'].notna()\n",
        "\n",
        "# Handle missing engagement\n",
        "df['total_usage'] = df['total_usage'].fillna(0)\n",
        "df['num_features'] = df['num_features'].fillna(0)\n",
        "\n",
        "print(f\"Days active: min={df['days_active'].min()}, max={df['days_active'].max()}\")\n",
        "print(f\"Total usage: min={df['total_usage'].min():.0f}, max={df['total_usage'].max():.0f}\")\n",
        "print(f\"Users with feature data: {(df['num_features'] > 0).sum()} / {len(df)}\")\n",
        "\n",
        "print(\"\\n3. FEATURE SCALING\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Before scaling\n",
        "print(f\"Total usage (before): mean={df['total_usage'].mean():.1f}, std={df['total_usage'].std():.1f}\")\n",
        "print(f\"Days active (before): mean={df['days_active'].mean():.1f}, std={df['days_active'].std():.1f}\")\n",
        "\n",
        "# Apply scaling\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(df[['total_usage', 'days_active']])\n",
        "\n",
        "print(f\"\\nTotal usage (after): mean={scaled_features[:,0].mean():.3f}, std={scaled_features[:,0].std():.3f}\")\n",
        "print(f\"Days active (after): mean={scaled_features[:,1].mean():.3f}, std={scaled_features[:,1].std():.3f}\")\n",
        "\n",
        "print(\"\\n4. CHURN PREDICTION FEATURE SET\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Churn rate overall: {df['is_churned'].mean():.1%}\")\n",
        "print(f\"Churn rate (high feature users): {df[df['num_features'] > 0]['is_churned'].mean():.1%}\")\n",
        "print(f\"Churn rate (low feature users): {df[df['num_features'] == 0]['is_churned'].mean():.1%}\")\n",
        "print(\"\\nInsight: Feature adoption is protective against churn!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Key Concepts ‚Äî Feature Engineering Best Practices\n",
        "\n",
        "### The Feature Engineering Hierarchy\n",
        "1. **Data collection**: Ensure you have the right data\n",
        "2. **Data cleaning**: Handle missing, duplicates, outliers\n",
        "3. **Domain features**: Leverage business knowledge (recency, frequency, customer lifecycle)\n",
        "4. **Statistical features**: Interactions, ratios, transformations\n",
        "5. **Automated features**: Deep learning, AutoML (usually unnecessary for SaaS)\n",
        "\n",
        "### Feature Quality Checklist\n",
        "- [ ] No missing values (or strategy documented)\n",
        "- [ ] Appropriate for model type (tree vs linear)\n",
        "- [ ] Interpretable to business stakeholders\n",
        "- [ ] Not highly correlated with other features\n",
        "- [ ] Not a data leak (information from the future)\n",
        "- [ ] Distribution makes sense (outliers justified)\n",
        "\n",
        "### Data Leakage: The Silent Killer\n",
        "```python\n",
        "# BAD: Using future information\n",
        "df['has_churn_flag'] = df['churn_date'].notna()  # we're predicting this!\n",
        "\n",
        "# GOOD: Using only historical information\n",
        "df['days_since_signup'] = (today - df['signup_date']).days\n",
        "```\n",
        "\n",
        "## ü§î Reflection & Application\n",
        "\n",
        "**Question 1:** Which single chart would you show a CEO in 30 seconds?\n",
        "- Line chart: Trend (retention vs churn over time)\n",
        "- Bar chart: Comparison (segment A vs B)\n",
        "- Combination: Show both signal and uncertainty\n",
        "\n",
        "**Question 2:** Should you include all features in your model?\n",
        "- No! Too many features ‚Üí overfitting ‚Üí poor generalization\n",
        "- Use correlation analysis, feature importance, or domain knowledge to select\n",
        "\n",
        "**Question 3:** How do you avoid data leakage in production?\n",
        "- Clearly timestamp each data point\n",
        "- Use only features available at decision time\n",
        "- Test on truly holdout (future) data\n",
        "\n",
        "## üìù Practice Assignment\n",
        "\n",
        "**Problem:** Create a customer quality score (0-100) combining:\n",
        "1. Engagement: Feature adoption and usage\n",
        "2. Stability: Customer lifetime and churn risk\n",
        "3. Value: Plan tier and payment health\n",
        "\n",
        "**Steps:**\n",
        "1. Engineer the 3 dimension features\n",
        "2. Normalize each to 0-100 scale\n",
        "3. Create composite score (weighted average)\n",
        "4. Validate: does high score correlate with lower churn?\n",
        "\n",
        "## üîó Next Steps\n",
        "\n",
        "In Week 6, we'll use these engineered features to train classification models that predict which customers are at risk."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
