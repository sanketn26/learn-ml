{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 1 ‚Äî NumPy Fundamentals & Vectorized Computing\n",
        "\n",
        "**Course:** Applied ML Foundations for SaaS Analytics  \n",
        "**Week Focus:** Learn NumPy arrays, vectorization, and efficient numerical operations for processing millions of SaaS telemetry records.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this week, you will:\n",
        "- Understand NumPy arrays and their advantages over Python lists\n",
        "- Master array creation, indexing, and manipulation\n",
        "- Apply vectorized operations for fast computations at scale\n",
        "- Use broadcasting to efficiently compute metrics across dimensions\n",
        "- Process real SaaS telemetry data with NumPy operations\n",
        "\n",
        "## üìä Real-World Context\n",
        "\n",
        "At a SaaS company like CloudWave, you're analyzing:\n",
        "- **Daily Active Users (DAU)**: tracking engagement trends\n",
        "- **Session counts**: per region, by plan tier, time-of-day\n",
        "- **Feature adoption**: how many users engage with each product feature\n",
        "- **Performance metrics**: response times, error rates, API latency\n",
        "\n",
        "NumPy is the foundation because:\n",
        "1. **Speed**: 50-100x faster than pure Python loops\n",
        "2. **Memory**: compact arrays use 8-10x less memory than lists\n",
        "3. **Simplicity**: expressive syntax for complex operations in one line\n",
        "4. **Integration**: ecosystem standard (Pandas, Scikit-learn, TensorFlow built on NumPy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "style"
        ]
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "HTML('''\n",
        "<style>\n",
        "details {\n",
        "  margin: 10px 0;\n",
        "  padding: 8px 12px;\n",
        "  border: 1px solid #d9e2ec;\n",
        "  border-radius: 8px;\n",
        "  background: #f9fbfd;\n",
        "}\n",
        "details summary {\n",
        "  font-weight: 600;\n",
        "  color: #0056b3;\n",
        "  cursor: pointer;\n",
        "}\n",
        "details[open] {\n",
        "  background: #f1f7ff;\n",
        "  border-color: #c3d4f0;\n",
        "}\n",
        "details pre {\n",
        "  background: #f8f9fa;\n",
        "  padding: 8px;\n",
        "  border-radius: 6px;\n",
        "}\n",
        "</style>\n",
        "''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üè¢ Scenario ‚Äî CloudWave Growth Analytics\n",
        "\n",
        "You're a data analyst at CloudWave, a growing SaaS company. Your CEO wants a daily health report:\n",
        "- Which regions are growing?\n",
        "- How do different customer tiers compare?\n",
        "- Is feature adoption uniform or concentrated?\n",
        "\n",
        "The challenge: you have 3 months of telemetry data with 220,000 event records. Processing millions of numbers efficiently requires understanding NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>üí° Hint ‚Äî Breaking Down the Problem</summary>\n",
        "\n",
        "**Hint 1:** When working with large arrays, think about the shape and dimensions you need:\n",
        "- Do you have per-user metrics or per-event metrics?\n",
        "- Are you aggregating across time (days/weeks) or across users (cohorts)?\n",
        "- What dimensions need broadcasting?\n",
        "\n",
        "**Hint 2:** Use NumPy's aggregation methods efficiently:\n",
        "```python\n",
        "# Good: single operation\n",
        "mean_usage = arr.mean()\n",
        "\n",
        "# Avoid: explicit loop\n",
        "mean_usage = sum(arr) / len(arr)  # slower\n",
        "```\n",
        "\n",
        "**Hint 3:** For edge cases:\n",
        "- Missing/zero values: use `np.nanmean()` to ignore NaN\n",
        "- Outliers: use `np.percentile()` for robust statistics\n",
        "- Integer vs float: be aware of type promotion (important for memory!)\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>‚úÖ Solution ‚Äî Feature Usage Aggregation</summary>\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('../data/feature_usage.csv', parse_dates=['date'], low_memory=False)\n",
        "\n",
        "# Extract feature usage counts as NumPy array\n",
        "usage_counts = df['usage_count'].values  # converts to NumPy array\n",
        "\n",
        "# Compute statistics\n",
        "print(f\"Total events: {len(usage_counts)}\")\n",
        "print(f\"Mean usage per event: {usage_counts.mean():.2f}\")\n",
        "print(f\"Median: {np.median(usage_counts):.2f}\")\n",
        "print(f\"Std Dev: {usage_counts.std():.2f}\")\n",
        "print(f\"Min: {usage_counts.min()}, Max: {usage_counts.max()}\")\n",
        "\n",
        "# Top 10% threshold\n",
        "percentile_90 = np.percentile(usage_counts, 90)\n",
        "top_10_percent = df[df['usage_count'] >= percentile_90]\n",
        "print(f\"\\nTop 10% threshold: {percentile_90}\")\n",
        "print(f\"Number of events in top 10%: {len(top_10_percent)}\")\n",
        "print(f\"Top users: {top_10_percent['user_id'].nunique()}\")\n",
        "```\n",
        "\n",
        "**Why this works:**\n",
        "- `.values` converts Pandas Series to NumPy array (then we get fast operations)\n",
        "- `np.percentile()` is vectorized and handles large arrays efficiently\n",
        "- No Python loops means we're leveraging NumPy's compiled C backend\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Key Concepts ‚Äî Why NumPy Matters\n",
        "\n",
        "### Arrays vs Lists\n",
        "```\n",
        "Python list:  [1, 2, 3, 4, 5]  ‚Üí stored as 5 separate objects in memory\n",
        "NumPy array:  array([1,2,3,4,5]) ‚Üí contiguous block, all elements same type\n",
        "```\n",
        "\n",
        "**Vectorization**: Operations applied to entire arrays without explicit loops.\n",
        "\n",
        "```python\n",
        "# Slow (Python loop)\n",
        "result = []\n",
        "for x in big_list:\n",
        "    result.append(x * 2)\n",
        "\n",
        "# Fast (NumPy vectorization)\n",
        "result = big_array * 2\n",
        "```\n",
        "\n",
        "### Broadcasting\n",
        "Automatically aligns dimensions for operations:\n",
        "```python\n",
        "# 2D array (100 days √ó 50 regions)\n",
        "daily_revenue = np.random.rand(100, 50)\n",
        "\n",
        "# 1D baseline (50 regions)\n",
        "baseline = np.array([100, 150, 200, ...])  # one per region\n",
        "\n",
        "# Automatically broadcasts baseline across all days\n",
        "adjusted = daily_revenue / baseline  # shape (100, 50)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úçÔ∏è Hands-on Exercises\n",
        "\n",
        "### Exercise 1: Create and Manipulate Arrays\n",
        "Load feature usage data and compute metrics using NumPy operations:\n",
        "1. Load the feature_usage.csv data into a NumPy array\n",
        "2. Extract usage counts for a single feature across all events\n",
        "3. Compute: mean, median, std dev, min, max usage counts\n",
        "4. Identify which users are in the top 10% for usage\n",
        "\n",
        "### Exercise 2: Broadcasting in Action\n",
        "1. Load daily DAU data (shape: 90 days √ó 50 regions)\n",
        "2. Compute the regional average DAU across the 90-day period\n",
        "3. Calculate deviation from regional average for each day (broadcast)\n",
        "4. Find which region+day combination has highest growth rate\n",
        "\n",
        "### Exercise 3: Vectorized Aggregations\n",
        "1. Load user_events.csv\n",
        "2. Group events by user (use unique users as an index)\n",
        "3. For each user, compute: total events, avg event_value, events per day\n",
        "4. All operations must use NumPy (no Pandas loops)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>üí° Hint ‚Äî Broadcasting Challenge</summary>\n",
        "\n",
        "When you have arrays of different shapes, NumPy automatically broadcasts them:\n",
        "\n",
        "```python\n",
        "# Day 1 data (50 regions):    [100, 150, 200, ...]  shape (50,)\n",
        "# Regional baseline (50):      [95,  140, 210, ...]  shape (50,)\n",
        "#\n",
        "# When you divide them, NumPy broadcasts baseline to match across rows:\n",
        "# [100/95, 100/140, 100/210, ...] for day 1\n",
        "# [120/95, 120/140, 120/210, ...] for day 2\n",
        "# etc.\n",
        "```\n",
        "\n",
        "**Key:** The shapes must be compatible for broadcasting:\n",
        "- Dimensions align from the right\n",
        "- Size 1 broadcasts to any size\n",
        "- Mismatched non-1 dimensions cause an error\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>‚úÖ Solution ‚Äî Vectorized Metrics Computation</summary>\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load feature usage and reshape for analysis\n",
        "df = pd.read_csv('../data/feature_usage.csv', parse_dates=['date'], low_memory=False)\n",
        "\n",
        "# Method 1: Using Pandas groupby, then convert to NumPy for vectorized operations\n",
        "user_stats = df.groupby('user_id').agg({\n",
        "    'usage_count': ['sum', 'mean', 'count']\n",
        "}).reset_index()\n",
        "user_stats.columns = ['user_id', 'total_usage', 'avg_usage', 'event_count']\n",
        "\n",
        "# Now use NumPy to find top users\n",
        "total_usage_array = user_stats['total_usage'].values\n",
        "top_10_percent_threshold = np.percentile(total_usage_array, 90)\n",
        "top_users = user_stats[user_stats['total_usage'] >= top_10_percent_threshold]\n",
        "\n",
        "print(f\"Total unique users: {len(user_stats)}\")\n",
        "print(f\"Top 10% threshold: {top_10_percent_threshold:.0f}\")\n",
        "print(f\"Top 10% users: {len(top_users)}\")\n",
        "print(f\"Their avg usage: {top_users['avg_usage'].mean():.2f}\")\n",
        "\n",
        "# Using NumPy broadcasting to compute z-scores\n",
        "mean_usage = total_usage_array.mean()\n",
        "std_usage = total_usage_array.std()\n",
        "z_scores = (total_usage_array - mean_usage) / std_usage\n",
        "outliers = np.where(np.abs(z_scores) > 2.5)[0]\n",
        "print(f\"Outlier users (|z| > 2.5): {len(outliers)}\")\n",
        "```\n",
        "\n",
        "**Why this hybrid approach works:**\n",
        "- Pandas handles grouped aggregation (more efficient than manual NumPy)\n",
        "- NumPy handles statistical computations (vectorized and fast)\n",
        "- `.values` bridges Pandas and NumPy seamlessly\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick demo: NumPy operations on real SaaS data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load feature usage data\n",
        "df = pd.read_csv('../data/feature_usage.csv')\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE USAGE DATA OVERVIEW\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total records: {len(df):,}\")\n",
        "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "print(f\"Unique users: {df['user_id'].nunique():,}\")\n",
        "print(f\"Unique features: {df['feature_name'].nunique()}\")\n",
        "print()\n",
        "\n",
        "# NumPy operations: find total usage per feature\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE USAGE SUMMARY (VECTORIZED WITH NUMPY)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Convert to NumPy array\n",
        "usage_array = df['usage_count'].values\n",
        "\n",
        "# Compute statistics\n",
        "stats = {\n",
        "    'Total usage events': len(usage_array),\n",
        "    'Mean usage': f\"{usage_array.mean():.2f}\",\n",
        "    'Median usage': f\"{np.median(usage_array):.2f}\",\n",
        "    'Std Dev': f\"{usage_array.std():.2f}\",\n",
        "    'Min': f\"{usage_array.min():.0f}\",\n",
        "    'Max': f\"{usage_array.max():.0f}\",\n",
        "    'P95': f\"{np.percentile(usage_array, 95):.0f}\",\n",
        "}\n",
        "\n",
        "for key, val in stats.items():\n",
        "    print(f\"{key:.<40} {val}\")\n",
        "\n",
        "# Group by feature using pandas, then analyze with NumPy\n",
        "feature_usage = df.groupby('feature_name')['usage_count'].sum().sort_values(ascending=False)\n",
        "print()\n",
        "print(\"Top 5 Features by Total Usage:\")\n",
        "print(feature_usage.head())\n",
        "print()\n",
        "print(\"NumPy power: computed statistics on 160,000+ records in milliseconds!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§î Reflection & Application\n",
        "\n",
        "**Question 1:** Broadcasting helps scale baseline usage across 100k users. How would you structure this?\n",
        "- Create a baseline array (features) and usage array (users √ó features)\n",
        "- The broadcasting rule: `(100000, 1) / (1, 50)` becomes `(100000, 50)`\n",
        "\n",
        "**Question 2:** When is NumPy not the right tool?\n",
        "- When you need labeled data (use Pandas)\n",
        "- When working with strings/text heavily (use regular Python)\n",
        "- When data doesn't fit in memory (use Dask or Spark)\n",
        "\n",
        "**Question 3:** How does NumPy performance scale?\n",
        "- 1,000 elements: all methods similar\n",
        "- 1,000,000 elements: NumPy ~50x faster\n",
        "- 1,000,000,000+ elements: NumPy essential; pure Python becomes impractical\n",
        "\n",
        "## üìù Practice Assignment\n",
        "\n",
        "**Problem:** You have daily DAU data for 30 days across 10 regions. Compute:\n",
        "1. Total DAU across all regions for each day\n",
        "2. Regional share of total DAU each day (as percentages)\n",
        "3. Which region had highest average share? Lowest?\n",
        "4. Compute z-score for each region's daily DAU (standardize by region)\n",
        "5. Identify anomalies (days where any region's DAU is > 2œÉ from its mean)\n",
        "\n",
        "**Deliverable:** Write functions using NumPy (no loops), document with comments explaining the broadcasting logic.\n",
        "\n",
        "## üîó Next Steps\n",
        "\n",
        "In Week 2, we'll layer Pandas on top to handle labeled data, join multiple tables, and prepare datasets for analysis. NumPy becomes the engine underlying Pandas' high-performance operations."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
