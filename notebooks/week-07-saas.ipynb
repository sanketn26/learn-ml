{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 ‚Äî Supervised Learning: Regression\n",
    "\n",
    "**Course:** Applied ML Foundations for SaaS Analytics  \n",
    "**Week Focus:** Predict continuous values‚Äîcustomer lifetime value, revenue, usage trends‚Äîwith regression models.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this week, you will:\n",
    "- Build and evaluate regression models (Linear, Ridge, Random Forest Regressor)\n",
    "- Interpret regression coefficients and feature effects\n",
    "- Handle heteroscedasticity and non-linear relationships\n",
    "- Predict customer lifetime value (CLV) and monthly recurring revenue (MRR)\n",
    "- Optimize for business metrics (MAE, RMSE, R¬≤ in business context)\n",
    "- Understand when to use regression vs classification\n",
    "\n",
    "## üìä Real-World Context\n",
    "\n",
    "At a SaaS company like CloudWave, you need to predict:\n",
    "- **Customer Lifetime Value (CLV)**: How much revenue will this customer generate?\n",
    "- **Monthly Recurring Revenue (MRR)**: What's our projected monthly revenue?\n",
    "- **Usage Trends**: How many API calls will customers make next month?\n",
    "- **Support Costs**: How much will customer support cost per account?\n",
    "\n",
    "**Business Impact:**\n",
    "- üí∞ Better resource allocation based on predicted revenue\n",
    "- üéØ Target high-value customers for upselling\n",
    "- üìä Accurate financial forecasting for investors\n",
    "- ‚ö†Ô∏è Identify customers with declining usage before they churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''\n",
    "<style>\n",
    "details {\n",
    "  margin: 10px 0;\n",
    "  padding: 8px 12px;\n",
    "  border: 1px solid #d9e2ec;\n",
    "  border-radius: 8px;\n",
    "  background: #f9fbfd;\n",
    "}\n",
    "details summary {\n",
    "  font-weight: 600;\n",
    "  color: #0056b3;\n",
    "  cursor: pointer;\n",
    "}\n",
    "details[open] {\n",
    "  background: #f1f7ff;\n",
    "  border-color: #c3d4f0;\n",
    "}\n",
    "details pre {\n",
    "  background: #f8f9fa;\n",
    "  padding: 8px;\n",
    "  border-radius: 6px;\n",
    "}\n",
    "</style>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè¢ Scenario ‚Äî Predicting Customer Lifetime Value\n",
    "\n",
    "Your CFO asks: **\"How much revenue will each customer generate over their lifetime?\"**\n",
    "\n",
    "**Why CLV Matters:**\n",
    "- Determine how much to spend on customer acquisition\n",
    "- Prioritize high-value customers for retention efforts\n",
    "- Set pricing and discount strategies\n",
    "- Make data-driven investment decisions\n",
    "\n",
    "**The Challenge:**\n",
    "- Classification predicts categories (will churn: yes/no)\n",
    "- **Regression predicts numbers** (will generate: $X revenue)\n",
    "- Need to estimate a continuous value, not a discrete label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint ‚Äî Regression vs Classification</summary>\n",
    "\n",
    "**When to use Classification:**\n",
    "- Binary outcome: Will churn? (Yes/No)\n",
    "- Categories: Plan tier? (Free/Pro/Enterprise)\n",
    "- Discrete labels: Support priority? (Low/Medium/High)\n",
    "\n",
    "**When to use Regression:**\n",
    "- Continuous values: Customer Lifetime Value? ($0 - $100,000)\n",
    "- Counts: API calls next month? (0 - 1,000,000)\n",
    "- Percentages: Engagement score? (0% - 100%)\n",
    "- Time: Days until churn? (0 - 365)\n",
    "\n",
    "**Key Difference:**\n",
    "- Classification: Predict a category\n",
    "- Regression: Predict a number\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Part 1: Building Regression Features\n",
    "\n",
    "For predicting CLV, we need features that capture:\n",
    "1. **Engagement**: How actively they use the product\n",
    "2. **Tenure**: How long they've been a customer\n",
    "3. **Plan**: Current pricing tier\n",
    "4. **Growth**: Trend in usage over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Load subscription data\n",
    "subs = pd.read_csv('../data/subscriptions.csv', parse_dates=['signup_date', 'churn_date'])\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SUBSCRIPTION DATA OVERVIEW\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total customers: {len(subs):,}\")\n",
    "print(f\"Active customers: {subs['churn_date'].isna().sum():,}\")\n",
    "print(f\"Churned customers: {subs['churn_date'].notna().sum():,}\")\n",
    "print(f\"\\nPlan distribution:\")\n",
    "print(subs['plan_tier'].value_counts())\n",
    "print(f\"\\nMRR statistics:\")\n",
    "print(subs['mrr'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for CLV Prediction\n",
    "\n",
    "# 1. Calculate customer lifetime in days\n",
    "today = pd.Timestamp.now()\n",
    "subs['end_date'] = subs['churn_date'].fillna(today)\n",
    "subs['lifetime_days'] = (subs['end_date'] - subs['signup_date']).dt.days\n",
    "\n",
    "# 2. Calculate total revenue (CLV for churned customers, current for active)\n",
    "subs['lifetime_months'] = subs['lifetime_days'] / 30.0\n",
    "subs['lifetime_value'] = subs['mrr'] * subs['lifetime_months']\n",
    "\n",
    "# 3. Load engagement data\n",
    "feature_usage = pd.read_csv('../data/feature_usage.csv')\n",
    "user_events = pd.read_csv('../data/user_events.csv')\n",
    "\n",
    "# Aggregate engagement metrics\n",
    "engagement = feature_usage.groupby('user_id').agg({\n",
    "    'usage_count': 'sum',\n",
    "    'feature_name': 'nunique'\n",
    "}).rename(columns={\n",
    "    'usage_count': 'total_usage',\n",
    "    'feature_name': 'features_used'\n",
    "}).reset_index()\n",
    "\n",
    "# Event frequency\n",
    "events = user_events.groupby('user_id').size().reset_index(name='total_events')\n",
    "\n",
    "# 4. Merge features\n",
    "df = subs.merge(engagement, on='user_id', how='left')\n",
    "df = df.merge(events, on='user_id', how='left')\n",
    "\n",
    "# Fill missing values\n",
    "df['total_usage'] = df['total_usage'].fillna(0)\n",
    "df['features_used'] = df['features_used'].fillna(0)\n",
    "df['total_events'] = df['total_events'].fillna(0)\n",
    "\n",
    "# 5. Create plan tier dummies\n",
    "df = pd.get_dummies(df, columns=['plan_tier'], drop_first=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Final dataset: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"\\nFeatures for modeling:\")\n",
    "feature_cols = ['tenure_days', 'mrr', 'total_usage', 'features_used', 'total_events']\n",
    "feature_cols += [col for col in df.columns if col.startswith('plan_tier_')]\n",
    "print(f\"  {feature_cols}\")\n",
    "print(f\"\\nTarget variable: lifetime_value\")\n",
    "print(f\"  Mean CLV: ${df['lifetime_value'].mean():.2f}\")\n",
    "print(f\"  Median CLV: ${df['lifetime_value'].median():.2f}\")\n",
    "print(f\"  Max CLV: ${df['lifetime_value'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 2: Training Regression Models\n",
    "\n",
    "We'll train three types of regression models:\n",
    "\n",
    "1. **Linear Regression**: Simple, interpretable baseline\n",
    "2. **Ridge Regression**: Linear with regularization (handles multicollinearity)\n",
    "3. **Random Forest Regressor**: Non-linear, captures complex relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = ['tenure_days', 'mrr', 'total_usage', 'features_used', 'total_events']\n",
    "feature_cols += [col for col in df.columns if col.startswith('plan_tier_')]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['lifetime_value']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING REGRESSION MODELS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training set: {len(X_train):,} customers\")\n",
    "print(f\"Test set: {len(X_test):,} customers\")\n",
    "print(f\"Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Linear Regression\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"1. LINEAR REGRESSION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"MAE:  ${mae_lr:,.2f}\")\n",
    "print(f\"RMSE: ${rmse_lr:,.2f}\")\n",
    "print(f\"R¬≤:   {r2_lr:.4f}\")\n",
    "\n",
    "print(f\"\\nTop 3 Feature Coefficients:\")\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'coefficient': lr.coef_\n",
    "}).sort_values('coefficient', ascending=False, key=abs)\n",
    "\n",
    "for idx, row in coef_df.head(3).iterrows():\n",
    "    print(f\"  {row['feature']:.<30} ${row['coefficient']:>10,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Ridge Regression (with regularization)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2. RIDGE REGRESSION (L2 Regularization)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "mae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(f\"MAE:  ${mae_ridge:,.2f}\")\n",
    "print(f\"RMSE: ${rmse_ridge:,.2f}\")\n",
    "print(f\"R¬≤:   {r2_ridge:.4f}\")\n",
    "print(f\"\\nüí° Ridge helps when features are correlated (multicollinearity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Random Forest Regressor (non-linear)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"3. RANDOM FOREST REGRESSOR\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"MAE:  ${mae_rf:,.2f}\")\n",
    "print(f\"RMSE: ${rmse_rf:,.2f}\")\n",
    "print(f\"R¬≤:   {r2_rf:.4f}\")\n",
    "\n",
    "print(f\"\\nFeature Importance (Top 5):\")\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "for idx, row in importance_df.head(5).iterrows():\n",
    "    print(f\"  {row['feature']:.<30} {row['importance']:>6.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Ridge Regression', 'Random Forest'],\n",
    "    'MAE ($)': [mae_lr, mae_ridge, mae_rf],\n",
    "    'RMSE ($)': [rmse_lr, rmse_ridge, rmse_rf],\n",
    "    'R¬≤': [r2_lr, r2_ridge, r2_rf]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "print(f\"\\n‚úÖ Best model: {comparison.loc[comparison['R¬≤'].idxmax(), 'Model']}\")\n",
    "print(f\"   (Highest R¬≤ = {comparison['R¬≤'].max():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Part 3: Understanding Regression Metrics\n",
    "\n",
    "### Key Metrics Explained\n",
    "\n",
    "**1. MAE (Mean Absolute Error)**\n",
    "- Average absolute prediction error\n",
    "- Example: MAE = $500 means \"on average, we're off by $500\"\n",
    "- ‚úÖ Easy to interpret in business terms\n",
    "- ‚úÖ Not sensitive to outliers\n",
    "\n",
    "**2. RMSE (Root Mean Squared Error)**\n",
    "- Square root of average squared error\n",
    "- ‚ùå Penalizes large errors more heavily\n",
    "- Example: RMSE = $800 (worse than MAE = $500) means some predictions are way off\n",
    "\n",
    "**3. R¬≤ (R-squared / Coefficient of Determination)**\n",
    "- Proportion of variance explained by the model\n",
    "- Range: 0 to 1 (higher is better)\n",
    "- R¬≤ = 0.75 means \"model explains 75% of the variance in CLV\"\n",
    "- Remaining 25% is unexplained (randomness, missing features)\n",
    "\n",
    "### Business Interpretation\n",
    "\n",
    "**Scenario:** Predicting CLV for customers with actual CLV ranging from $0 - $50,000\n",
    "\n",
    "- **MAE = $2,000**: On average, predictions are within $2,000 of actual\n",
    "- **R¬≤ = 0.80**: Model captures 80% of CLV variation\n",
    "- **Business Value**: Good enough for segmentation (high/medium/low value customers)\n",
    "- **Not Good Enough For**: Exact revenue forecasting (need MAE < $500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint ‚Äî When is R¬≤ Too Low?</summary>\n",
    "\n",
    "**R¬≤ Interpretation Guidelines:**\n",
    "\n",
    "- **R¬≤ > 0.90**: Excellent (rare in business data)\n",
    "- **R¬≤ = 0.70-0.90**: Good (explains most variance)\n",
    "- **R¬≤ = 0.50-0.70**: Moderate (useful for segmentation)\n",
    "- **R¬≤ = 0.30-0.50**: Weak (better than random guessing)\n",
    "- **R¬≤ < 0.30**: Poor (model adds little value)\n",
    "\n",
    "**Context Matters:**\n",
    "- Predicting physics: Expect R¬≤ > 0.95\n",
    "- Predicting customer behavior: R¬≤ = 0.60 is often very good!\n",
    "- Human decisions are inherently noisy\n",
    "\n",
    "**When R¬≤ is Low:**\n",
    "1. Missing important features\n",
    "2. Non-linear relationship (try Random Forest)\n",
    "3. High natural variance in target variable\n",
    "4. Insufficient data\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíº Part 4: Business Application\n",
    "\n",
    "### Use Case: Customer Segmentation by Predicted CLV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best model (Random Forest) to predict CLV for all customers\n",
    "df['predicted_clv'] = rf.predict(X)\n",
    "\n",
    "# Segment customers by predicted CLV\n",
    "df['clv_segment'] = pd.cut(\n",
    "    df['predicted_clv'],\n",
    "    bins=[0, 5000, 15000, 100000],\n",
    "    labels=['Low Value', 'Medium Value', 'High Value']\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CUSTOMER SEGMENTATION BY PREDICTED CLV\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "segment_summary = df.groupby('clv_segment').agg({\n",
    "    'user_id': 'count',\n",
    "    'predicted_clv': 'mean',\n",
    "    'lifetime_value': 'mean',\n",
    "    'mrr': 'mean',\n",
    "    'features_used': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "segment_summary.columns = ['Count', 'Predicted CLV', 'Actual CLV', 'Avg MRR', 'Avg Features']\n",
    "print(segment_summary)\n",
    "\n",
    "print(f\"\\nüí° Business Actions:\")\n",
    "print(f\"   High Value ({segment_summary.loc['High Value', 'Count']:.0f} customers):\")\n",
    "print(f\"      ‚Üí Assign dedicated account managers\")\n",
    "print(f\"      ‚Üí Offer premium support and custom features\")\n",
    "print(f\"   Medium Value ({segment_summary.loc['Medium Value', 'Count']:.0f} customers):\")\n",
    "print(f\"      ‚Üí Upsell campaigns to move to high value\")\n",
    "print(f\"      ‚Üí Feature adoption programs\")\n",
    "print(f\"   Low Value ({segment_summary.loc['Low Value', 'Count']:.0f} customers):\")\n",
    "print(f\"      ‚Üí Automated onboarding and self-service support\")\n",
    "print(f\"      ‚Üí Monitor for churn signals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Reflection & Application\n",
    "\n",
    "**Question 1:** Why might Random Forest outperform Linear Regression for CLV prediction?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "**Non-linear Relationships:**\n",
    "- CLV doesn't increase linearly with features\n",
    "- Example: First 5 features used ‚Üí huge CLV boost. Next 5 ‚Üí diminishing returns\n",
    "- Linear models assume straight-line relationships\n",
    "- Random Forest captures curves, thresholds, interactions\n",
    "\n",
    "**Feature Interactions:**\n",
    "- High MRR + high usage ‚Üí very high CLV (multiplicative effect)\n",
    "- Linear models: `CLV = a√óMRR + b√óusage` (additive only)\n",
    "- Random Forest: Learns `CLV = f(MRR, usage)` where f can be any shape\n",
    "\n",
    "</details>\n",
    "\n",
    "**Question 2:** When should you use Linear Regression instead of Random Forest?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "**Use Linear Regression When:**\n",
    "1. **Interpretability is critical**: Need to explain \"$1 increase in MRR ‚Üí $30 increase in CLV\"\n",
    "2. **Small datasets**: < 1000 samples (Random Forest needs more data)\n",
    "3. **Extrapolation needed**: Predicting values outside training range\n",
    "4. **Regulatory requirements**: Finance/healthcare often require interpretable models\n",
    "5. **Baseline**: Always start simple, add complexity only if needed\n",
    "\n",
    "**Random Forest Advantages:**\n",
    "- Better predictions (usually)\n",
    "- Handles non-linearity\n",
    "- Less feature engineering needed\n",
    "- Built-in feature importance\n",
    "\n",
    "</details>\n",
    "\n",
    "**Question 3:** How do you know if your model is good enough for production?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "**Compare to Baselines:**\n",
    "1. **Naive baseline**: Predict the mean CLV for everyone\n",
    "   - If MAE_model ‚âà MAE_baseline ‚Üí model adds no value!\n",
    "2. **Business baseline**: Current method (manual estimates, rules)\n",
    "   - Model should improve on existing process\n",
    "\n",
    "**Business Value Check:**\n",
    "- What's the cost of a $2,000 prediction error?\n",
    "- If low impact ‚Üí R¬≤ = 0.50 is fine\n",
    "- If high stakes (financial decisions) ‚Üí need R¬≤ > 0.80\n",
    "\n",
    "**Test in Production:**\n",
    "- Deploy to 10% of customers\n",
    "- Compare predicted vs actual CLV after 3 months\n",
    "- If predictions hold up ‚Üí scale to 100%\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Hands-On Exercises\n",
    "\n",
    "### Exercise 1: Feature Engineering for Better Predictions\n",
    "\n",
    "Current features might be too simple. Create these advanced features:\n",
    "\n",
    "1. **Engagement Velocity**: Change in usage over last 30 days vs previous 30 days\n",
    "2. **Feature Diversity Score**: (features_used / total_available_features)\n",
    "3. **MRR per Event**: (mrr / total_events) ‚Äî efficiency metric\n",
    "4. **Cohort Age**: Months since account creation (tenure in months)\n",
    "\n",
    "Re-train your Random Forest and see if R¬≤ improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here!\n",
    "\n",
    "# TODO: Create new features\n",
    "# df['engagement_velocity'] = ...\n",
    "# df['feature_diversity'] = ...\n",
    "# df['mrr_per_event'] = ...\n",
    "# df['cohort_age_months'] = ...\n",
    "\n",
    "# TODO: Retrain model with new features\n",
    "# TODO: Compare R¬≤ before and after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Residual Analysis ‚Äî Finding Model Weaknesses\n",
    "\n",
    "Analyze prediction errors to find patterns:\n",
    "\n",
    "1. Calculate residuals: `residuals = y_test - y_pred`\n",
    "2. Plot residuals vs predicted values (should be random scatter)\n",
    "3. Find customers with largest errors (top 10 over-predictions and under-predictions)\n",
    "4. Analyze: What do these customers have in common?\n",
    "\n",
    "**Goal**: Identify systematic errors to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here!\n",
    "\n",
    "# TODO: Calculate residuals\n",
    "# TODO: Plot residuals (bonus: use matplotlib/seaborn)\n",
    "# TODO: Find top 10 worst predictions\n",
    "# TODO: Investigate common patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Time-Based Validation\n",
    "\n",
    "Our train/test split was random. For time-series data, we should train on past and test on future.\n",
    "\n",
    "1. Split data by time: Train on customers who signed up before 2024-06-01\n",
    "2. Test on customers who signed up after 2024-06-01\n",
    "3. Compare R¬≤ to random split ‚Äî is it lower? (It usually is!)\n",
    "4. Why? Discuss the difference between in-sample and out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here!\n",
    "\n",
    "# TODO: Create time-based train/test split\n",
    "# TODO: Train model on past data only\n",
    "# TODO: Test on future data\n",
    "# TODO: Compare to random split results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Practice Assignment\n",
    "\n",
    "**Problem:** Predict Monthly Recurring Revenue (MRR) 30 days into the future.\n",
    "\n",
    "**Dataset:** Use current engagement metrics to predict next month's MRR.\n",
    "\n",
    "**Steps:**\n",
    "1. Create target variable: `future_mrr` (MRR one month later)\n",
    "2. Features: Current usage, features adopted, event frequency, historical MRR trend\n",
    "3. Train Linear, Ridge, and Random Forest models\n",
    "4. Evaluate with MAE (in dollars) and R¬≤\n",
    "5. Identify customers with predicted MRR decrease > 20% (churn risk!)\n",
    "\n",
    "**Deliverable:** Notebook showing model comparison and business recommendations.\n",
    "\n",
    "**Bonus:** Build a dashboard showing:\n",
    "- Predicted vs Actual MRR for test set\n",
    "- Feature importance\n",
    "- Top 20 at-risk customers (declining MRR predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Next Steps\n",
    "\n",
    "In **Week 8**, we'll explore **Unsupervised Learning: Clustering** to segment customers without predefined labels. We'll discover natural groupings in customer behavior and create data-driven personas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
