{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 2 ‚Äî Pandas Data Manipulation & ETL\n",
        "\n",
        "**Course:** Applied ML Foundations for SaaS Analytics  \n",
        "**Week Focus:** Master Pandas for data cleaning, aggregation, joining multiple datasets, and preparing clean data for analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this week, you will:\n",
        "- Load and inspect data from multiple file formats (CSV, JSON)\n",
        "- Clean and handle missing data effectively\n",
        "- Group, aggregate, and pivot data for SaaS analytics\n",
        "- Join datasets to create comprehensive views\n",
        "- Detect and handle outliers and anomalies\n",
        "- Build reliable ETL pipelines\n",
        "\n",
        "## üìä Real-World Context\n",
        "\n",
        "SaaS analytics requires combining multiple data sources:\n",
        "- **Subscriptions data**: signup dates, churn dates, plan tier, pricing\n",
        "- **User events**: activity logs, timestamps, event types\n",
        "- **Feature usage**: which features users interact with\n",
        "- **Feedback**: customer sentiment, support tickets\n",
        "\n",
        "Your job: Transform messy, fragmented raw data into clean, joined datasets ready for analysis and modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "style"
        ]
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "HTML('''\n",
        "<style>\n",
        "details {\n",
        "  margin: 10px 0;\n",
        "  padding: 8px 12px;\n",
        "  border: 1px solid #d9e2ec;\n",
        "  border-radius: 8px;\n",
        "  background: #f9fbfd;\n",
        "}\n",
        "details summary {\n",
        "  font-weight: 600;\n",
        "  color: #0056b3;\n",
        "  cursor: pointer;\n",
        "}\n",
        "details[open] {\n",
        "  background: #f1f7ff;\n",
        "  border-color: #c3d4f0;\n",
        "}\n",
        "details pre {\n",
        "  background: #f8f9fa;\n",
        "  padding: 8px;\n",
        "  border-radius: 6px;\n",
        "}\n",
        "</style>\n",
        "''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üè¢ Scenario ‚Äî Merging CloudWave Data\n",
        "\n",
        "Your company stores data in 4 separate systems:\n",
        "1. **Subscriptions DB**: customer IDs, signup dates, churn dates, plan types\n",
        "2. **Event streaming logs**: user_id, timestamp, event_type, value\n",
        "3. **Feature DB**: feature_id, feature_name, category, launch_date\n",
        "4. **Customer feedback**: user_id, timestamp, category, sentiment, text\n",
        "\n",
        "**Task**: Create a unified \"customer 360\" view combining all sources to answer:\n",
        "- Which customers are at risk of churning?\n",
        "- Which features drive the most engagement?\n",
        "- How does feature adoption correlate with retention?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>üí° Hint ‚Äî Structured Approach to Data Integration</summary>\n",
        "\n",
        "**Step 1: Understand the data**\n",
        "- Load each dataset separately first\n",
        "- Check shapes, columns, data types\n",
        "- Look for key identifiers (user_id, feature_id, etc.)\n",
        "\n",
        "**Step 2: Data quality check**\n",
        "```python\n",
        "df.info()  # types and nulls\n",
        "df.describe()  # numeric summaries\n",
        "df[df.isnull().any(axis=1)]  # find rows with ANY null\n",
        "```\n",
        "\n",
        "**Step 3: Choose join strategy**\n",
        "- `inner`: only matching rows (stricter, smaller dataset)\n",
        "- `left`: keep all from left table, add from right\n",
        "- `outer`: keep all rows, fill NaNs where no match\n",
        "\n",
        "**Step 4: Validate results**\n",
        "- Check row count before/after\n",
        "- Verify no unexpected NaNs\n",
        "- Spot-check a few rows manually\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>‚úÖ Solution ‚Äî Subscription Lifecycle Analysis</summary>\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Load subscriptions\n",
        "subs = pd.read_csv('../data/subscriptions.csv', parse_dates=['signup_date','churn_date'])\n",
        "\n",
        "print(f\"Total subscriptions: {len(subs)}\")\n",
        "print(f\"Churned: {subs['churn_date'].notna().sum()}\")\n",
        "print(f\"Active: {subs['churn_date'].isna().sum()}\")\n",
        "\n",
        "# Create useful derived columns\n",
        "subs['is_churned'] = subs['churn_date'].notna()\n",
        "\n",
        "# Lifetime: days from signup to churn (or today if active)\n",
        "today = pd.Timestamp.now()\n",
        "subs['churn_or_today'] = subs['churn_date'].fillna(today)\n",
        "subs['lifetime_days'] = (subs['churn_or_today'] - subs['signup_date']).dt.days\n",
        "\n",
        "# Cohort by signup month\n",
        "subs['signup_month'] = subs['signup_date'].dt.to_period('M')\n",
        "\n",
        "# Churn rate by plan\n",
        "churn_by_plan = subs.groupby('plan_tier').agg({\n",
        "    'user_id': 'count',\n",
        "    'is_churned': 'sum'\n",
        "})\n",
        "churn_by_plan['churn_rate'] = churn_by_plan['is_churned'] / churn_by_plan['user_id']\n",
        "print(\"\\nChurn Rate by Plan:\")\n",
        "print(churn_by_plan)\n",
        "\n",
        "# Average lifetime by cohort\n",
        "lifetime_by_cohort = subs.groupby('signup_month')['lifetime_days'].mean()\n",
        "print(\"\\nAverage Lifetime (days) by Signup Cohort:\")\n",
        "print(lifetime_by_cohort)\n",
        "```\n",
        "\n",
        "**Key takeaways:**\n",
        "- Handle dates properly with `pd.to_datetime()`\n",
        "- Fill NaN values strategically for analysis\n",
        "- Groupby + agg is the engine for SaaS metrics\n",
        "- Create cohort columns for temporal analysis\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Key Concepts ‚Äî Pandas Fundamentals\n",
        "\n",
        "### DataFrames: The Core Abstraction\n",
        "```\n",
        "DataFrames = SQL tables + NumPy speed + Flexible labels\n",
        "```\n",
        "\n",
        "### Key Operations\n",
        "1. **Load**: `pd.read_csv()`, `pd.read_json()`, `pd.read_sql()`\n",
        "2. **Inspect**: `.shape`, `.dtypes`, `.info()`, `.describe()`, `.head()`\n",
        "3. **Clean**: `.dropna()`, `.fillna()`, `.astype()`\n",
        "4. **Group**: `.groupby()` ‚Üí powerful aggregations\n",
        "5. **Join**: `.merge()` ‚Üí combine datasets on keys\n",
        "6. **Pivot**: `.pivot_table()` ‚Üí reshape data for different views\n",
        "\n",
        "### Critical Patterns for SaaS\n",
        "\n",
        "```python\n",
        "# Find churn cohort\n",
        "df[df['churn_date'].notna()].groupby('signup_month')['user_id'].count()\n",
        "\n",
        "# Active users each day\n",
        "df[df['event_date'] >= '2024-01-01'].groupby(['event_date','user_id']).size()\n",
        "\n",
        "# Feature adoption by segment\n",
        "df.merge(feature_df, on='feature_id').groupby(['segment','feature_name'])['usage_count'].sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úçÔ∏è Hands-on Exercises\n",
        "\n",
        "### Exercise 1: Data Cleaning & Preparation\n",
        "1. Load subscriptions.csv\n",
        "2. Check for missing values and data types\n",
        "3. Convert signup_date and churn_date to datetime\n",
        "4. Create a \"is_churned\" binary column\n",
        "5. Create a \"lifetime_days\" metric (days from signup to churn or today)\n",
        "6. Handle any edge cases (negative lifetimes, invalid dates)\n",
        "\n",
        "### Exercise 2: Aggregation & Grouping\n",
        "1. Load user_events.csv and subscriptions.csv\n",
        "2. Join them on user_id\n",
        "3. For each user, compute: total events, first event date, last event date, event frequency\n",
        "4. Group by plan_tier: which tier has the highest event frequency?\n",
        "5. Create a cohort analysis: signup_month vs. retention at 30/60/90 days\n",
        "\n",
        "### Exercise 3: Multi-table Integration\n",
        "1. Load all 5 datasets\n",
        "2. Create a user-centric joined table with:\n",
        "   - Subscription info (plan_tier, signup_date, churn_date)\n",
        "   - Engagement metrics (total_events, total_feature_usage, feature_count)\n",
        "   - Feedback sentiment (avg_sentiment, feedback_count)\n",
        "3. Identify high-engagement, high-satisfaction customers at churn risk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>üí° Hint ‚Äî Joining Multiple Tables Effectively</summary>\n",
        "\n",
        "**Join types visualization:**\n",
        "```\n",
        "LEFT (subscription.user_id):  [1, 2, 3, 4, 5]\n",
        "RIGHT (events.user_id):       [2, 3, 4, 5, 6]\n",
        "\n",
        "INNER:  [2, 3, 4, 5]  ‚Üê only matching\n",
        "LEFT:   [1, 2, 3, 4, 5]  ‚Üê all from left, NaNs on right where no match\n",
        "OUTER:  [1, 2, 3, 4, 5, 6]  ‚Üê all from both\n",
        "```\n",
        "\n",
        "**Common pitfall:** Don't `.groupby()` after a join without checking cardinality!\n",
        "- If events table has multiple rows per user, you'll get multiplied results\n",
        "- Always aggregate at the appropriate level before joining\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>‚úÖ Solution ‚Äî Feature Adoption by Customer Segment</summary>\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load datasets\n",
        "subs = pd.read_csv('../data/subscriptions.csv', parse_dates=['signup_date','churn_date'])\n",
        "feature_usage = pd.read_csv('../data/feature_usage.csv')\n",
        "feature_cat = pd.read_csv('../data/product_catalog.csv')\n",
        "\n",
        "# Pre-aggregate: total usage per user per feature (avoid data explosion on join)\n",
        "user_feature_summary = (feature_usage\n",
        "    .groupby(['user_id','feature_name'])['usage_count']\n",
        "    .sum()\n",
        "    .reset_index()\n",
        "    .rename(columns={'usage_count':'total_usage'})\n",
        ")\n",
        "\n",
        "# Count features per user\n",
        "features_per_user = (user_feature_summary\n",
        "    .groupby('user_id')['feature_name']\n",
        "    .nunique()\n",
        "    .reset_index()\n",
        "    .rename(columns={'feature_name':'feature_count'})\n",
        ")\n",
        "\n",
        "# Join to subscriptions\n",
        "result = subs.merge(features_per_user, on='user_id', how='left')\n",
        "result['feature_count'] = result['feature_count'].fillna(0)\n",
        "\n",
        "# Analyze by plan tier\n",
        "adoption_by_plan = result.groupby('plan_tier').agg({\n",
        "    'feature_count': ['mean', 'median', 'std'],\n",
        "    'is_churned': 'mean'  # churn rate\n",
        "}).round(2)\n",
        "\n",
        "print(\"Feature adoption by plan tier:\")\n",
        "print(adoption_by_plan)\n",
        "\n",
        "# Do churned users have lower feature adoption?\n",
        "active = result[~result['is_churned']]\n",
        "churned = result[result['is_churned']]\n",
        "print(f\"\\nActive users avg features: {active['feature_count'].mean():.2f}\")\n",
        "print(f\"Churned users avg features: {churned['feature_count'].mean():.2f}\")\n",
        "```\n",
        "\n",
        "**Why this approach:**\n",
        "- Aggregates first to reduce join cardinality\n",
        "- Fills NaNs strategically (0 for no features = inactive)\n",
        "- Compares segments (active vs churned) to find patterns\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Pandas ETL pipeline for SaaS data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"WEEK 2: PANDAS DATA INTEGRATION DEMO\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Load subscriptions\n",
        "subs = pd.read_csv('../data/subscriptions.csv', parse_dates=['signup_date', 'churn_date'])\n",
        "print(f\"\\n1. SUBSCRIPTIONS DATA\")\n",
        "print(f\"   Records: {len(subs):,}\")\n",
        "print(f\"   Columns: {', '.join(subs.columns.tolist())}\")\n",
        "print(f\"   Churned: {subs['churn_date'].notna().sum():,} ({subs['churn_date'].notna().sum()/len(subs)*100:.1f}%)\")\n",
        "\n",
        "# Load events\n",
        "events = pd.read_csv('../data/user_events.csv')\n",
        "print(f\"\\n2. USER EVENTS DATA\")\n",
        "print(f\"   Records: {len(events):,}\")\n",
        "print(f\"   Unique users: {events['user_id'].nunique():,}\")\n",
        "print(f\"   Columns: {', '.join(events.columns.tolist())}\")\n",
        "\n",
        "# Load feature usage\n",
        "feature_usage = pd.read_csv('../data/feature_usage.csv')\n",
        "print(f\"\\n3. FEATURE USAGE DATA\")\n",
        "print(f\"   Records: {len(feature_usage):,}\")\n",
        "print(f\"   Unique features: {feature_usage['feature_name'].nunique()}\")\n",
        "print(f\"   Columns: {', '.join(feature_usage.columns.tolist())}\")\n",
        "\n",
        "# Create merged view\n",
        "print(f\"\\n4. DATA INTEGRATION\")\n",
        "\n",
        "# Aggregate user-level metrics\n",
        "user_stats = events.groupby('user_id').agg({\n",
        "    'event_type': 'count'\n",
        "}).rename(columns={'event_type':'total_events'}).reset_index()\n",
        "\n",
        "feature_stats = feature_usage.groupby('user_id').agg({\n",
        "    'usage_count': 'sum',\n",
        "    'feature_name': 'nunique'\n",
        "}).rename(columns={'usage_count':'total_usage', 'feature_name':'features_used'}).reset_index()\n",
        "\n",
        "# Join to subscriptions\n",
        "merged = subs.merge(user_stats, on='user_id', how='left')\n",
        "merged = merged.merge(feature_stats, on='user_id', how='left')\n",
        "\n",
        "print(f\"   Merged dataset: {len(merged):,} users\")\n",
        "print(f\"   Columns: {merged.shape[1]}\")\n",
        "\n",
        "# Analysis\n",
        "print(f\"\\n5. INSIGHTS FROM JOINED DATA\")\n",
        "merged['is_churned'] = merged['churn_date'].notna()\n",
        "print(f\"   Active users with data: {len(merged[merged['is_churned'] == False]):,}\")\n",
        "print(f\"   Churned users: {merged['is_churned'].sum():,}\")\n",
        "\n",
        "active = merged[~merged['is_churned']]\n",
        "churned = merged[merged['is_churned']]\n",
        "\n",
        "print(f\"\\n   Active users stats:\")\n",
        "print(f\"      Avg events: {active['total_events'].mean():.0f}\")\n",
        "print(f\"      Avg features used: {active['features_used'].mean():.1f}\")\n",
        "print(f\"\\n   Churned users stats:\")\n",
        "print(f\"      Avg events: {churned['total_events'].mean():.0f}\")\n",
        "print(f\"      Avg features used: {churned['features_used'].mean():.1f}\")\n",
        "print(f\"\\n   Insight: Churned users have lower feature adoption!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§î Reflection & Application\n",
        "\n",
        "**Question 1:** When should you use `.merge()` vs `.join()`?\n",
        "- `merge()`: explicit, works on any columns (not just index)\n",
        "- `join()`: default on index, slightly faster for index-based joins\n",
        "\n",
        "**Question 2:** What are the risks of `.merge()` without aggregating first?\n",
        "- Cardinality explosion: 10 subscriptions √ó 1000 events per user = 10,000 rows!\n",
        "- Solution: Aggregate to 1 row per user before joining\n",
        "\n",
        "**Question 3:** How do you validate a join was successful?\n",
        "- Check row count: should match the \"left\" table unless duplicates\n",
        "- Check NaN distribution: if many unexpected NaNs, join keys misaligned\n",
        "- Spot-check: manually verify a few rows\n",
        "\n",
        "## üìù Practice Assignment\n",
        "\n",
        "**Problem:** Create a customer risk score combining:\n",
        "1. Churn likelihood (cohort churn rate)\n",
        "2. Low engagement (below median feature adoption)\n",
        "3. Declining trend (compare recent usage to historical average)\n",
        "\n",
        "**Steps:**\n",
        "1. Load all datasets\n",
        "2. Create user-level aggregations for each dimension\n",
        "3. Merge into a unified customer table\n",
        "4. Compute a composite risk score (0-100)\n",
        "5. Segment users: low/medium/high risk\n",
        "\n",
        "**Deliverable:** Jupyter notebook showing the ETL process and final risk segment distribution.\n",
        "\n",
        "## üîó Next Steps\n",
        "\n",
        "In Week 3, we'll visualize these joined datasets to uncover patterns and tell compelling stories about customer behavior."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
