{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 11 \u2014 Deep Learning Fundamentals\n\n**Course:** Applied ML Foundations for SaaS Analytics  \n**Week Focus:** Neural networks, backpropagation, and deep learning for SaaS predictions.\n\n---\n\n## \ud83c\udfaf Learning Objectives\n\n- Understand neural network architecture\n- Implement feedforward networks with Keras/TensorFlow\n- Apply regularization (Dropout, L1/L2, Early Stopping)\n- Compare deep learning vs classical ML\n- Build production-ready neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd, numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load data\nsubs = pd.read_csv('../data/subscriptions.csv')\nfeature_usage = pd.read_csv('../data/feature_usage.csv')\nuser_events = pd.read_csv('../data/user_events.csv')\n\n# Feature engineering\nengagement = feature_usage.groupby('user_id').agg({'usage_count': 'sum', 'feature_name': 'nunique'}).reset_index()\nengagement.columns = ['user_id', 'total_usage', 'features_adopted']\nevents = user_events.groupby('user_id').size().reset_index(name='total_events')\n\ndf = subs[['user_id', 'tenure_days', 'mrr', 'churn_date']].merge(engagement, on='user_id', how='left')\ndf = df.merge(events, on='user_id', how='left').fillna(0)\ndf['churned'] = df['churn_date'].notna().astype(int)\n\nfeatures = ['tenure_days', 'mrr', 'total_usage', 'features_adopted', 'total_events']\nX = df[features]\ny = df['churned']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\nprint(f\"Dataset: {len(df)} customers | Features: {len(features)}\")\nprint(f\"Train: {len(X_train)} | Test: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Neural Network Basics\n\n**Structure:**\n- Input layer: Raw features\n- Hidden layers: Learn representations\n- Output layer: Prediction\n\n**Activation functions:** ReLU, Sigmoid, Tanh\n\n**\ud83d\udca1 Depth Note:** Why do we need activation functions? Explore vanishing gradients problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n    from tensorflow.keras import Sequential\n    from tensorflow.keras.layers import Dense, Dropout\n    from tensorflow.keras.callbacks import EarlyStopping\n    \n    # Simple neural network\n    model = Sequential([\n        Dense(32, activation='relu', input_dim=X_train.shape[1]),\n        Dense(16, activation='relu'),\n        Dense(8, activation='relu'),\n        Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    \n    print(\"Model architecture:\")\n    model.summary()\nexcept ImportError:\n    print('TensorFlow not installed. Install with: pip install tensorflow')\n    print('For CPU-only: pip install tensorflow-cpu')\n    print('For GPU: pip install tensorflow-gpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Regularization Techniques\n\n**Overfitting problem:** Model learns noise, poor generalization\n\n**Solutions:**\n- Dropout: Randomly deactivate neurons\n- L1/L2: Penalize large weights\n- Early Stopping: Stop when validation loss plateaus\n\n**\ud83d\udca1 Depth Note:** How much dropout? Compare different rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n    # Model with regularization\n    model_reg = Sequential([\n        Dense(64, activation='relu', input_dim=X_train.shape[1]),\n        Dropout(0.3),\n        Dense(32, activation='relu'),\n        Dropout(0.3),\n        Dense(16, activation='relu'),\n        Dropout(0.2),\n        Dense(1, activation='sigmoid')\n    ])\n    \n    model_reg.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    \n    # Train with early stopping\n    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    \n    print(\"Training model with regularization...\")\n    history = model_reg.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=50,\n        batch_size=32,\n        callbacks=[early_stop],\n        verbose=0\n    )\n    \n    # Evaluate\n    y_pred = (model_reg.predict(X_test, verbose=0) > 0.5).flatten()\n    auc = roc_auc_score(y_test, model_reg.predict(X_test, verbose=0))\n    print(f\"\\nRegularized Model - AUC: {auc:.4f}\")\nexcept Exception as e:\n    print(f'Could not train model: {e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Deep Learning vs Classical ML\n\n**Classical ML:** Interpretable, fast, works with small data\n\n**Deep Learning:** Better with large data, complex patterns, black box\n\n**\ud83d\udca1 Depth Note:** When should you use each? Compare on different dataset sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Classical baseline\nlr = LogisticRegression(random_state=42, max_iter=1000)\nlr.fit(X_train, y_train)\nlr_auc = roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1])\n\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\nrf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\n\nprint(\"Comparison:\")\nprint(f\"Logistic Regression: {lr_auc:.4f}\")\nprint(f\"Random Forest:       {rf_auc:.4f}\")\nprint(f\"\\n\ud83d\udcca For this dataset size ({len(df)} customers):\")\nprint(f\"   Classical ML performs similarly or better\")\nprint(f\"   Deep learning needs 100K+ examples to shine\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hands-On Exercises\n\n### Exercise 1: Hyperparameter Tuning\nTry different layer sizes, dropout rates, learning rates. What's optimal?\n\n### Exercise 2: Visualization\nPlot training/validation loss curves. What do they tell us?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Hyperparameter grid search\n# TODO: Plot learning curves\n# TODO: Compare architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n\n\u2705 Neural network fundamentals  \n\u2705 Activation functions and backpropagation  \n\u2705 Regularization for preventing overfitting  \n\u2705 When to use deep learning vs classical ML  \n\n## \ud83d\udd1c Next Week: Capstone Project"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}