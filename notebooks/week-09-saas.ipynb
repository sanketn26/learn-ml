{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 â€” Dimensionality Reduction: PCA\n",
    "\n",
    "**Course:** Applied ML Foundations for SaaS Analytics  \n",
    "**Week Focus:** Reduce high-dimensional data to 2-3 dimensions for visualization and denoising.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "- Understand Principal Component Analysis (PCA)\n",
    "- Reduce features while preserving variance\n",
    "- Interpret principal components\n",
    "- Use PCA for visualization and denoising\n",
    "- Balance information retention vs simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Load data\n",
    "subs = pd.read_csv('../data/subscriptions.csv')\n",
    "feature_usage = pd.read_csv('../data/feature_usage.csv')\n",
    "user_events = pd.read_csv('../data/user_events.csv')\n",
    "\n",
    "# Feature engineering\n",
    "engagement = feature_usage.groupby('user_id').agg({'usage_count': 'sum', 'feature_name': 'nunique'}).reset_index()\n",
    "engagement.columns = ['user_id', 'total_usage', 'features_adopted']\n",
    "\n",
    "events = user_events.groupby('user_id').size().reset_index(name='total_events')\n",
    "\n",
    "df = subs[['user_id', 'tenure_days', 'mrr']].merge(engagement, on='user_id', how='left')\n",
    "df = df.merge(events, on='user_id', how='left').fillna(0)\n",
    "\n",
    "# Derived features\n",
    "df['usage_per_day'] = df['total_usage'] / (df['tenure_days'] + 1)\n",
    "df['events_per_day'] = df['total_events'] / (df['tenure_days'] + 1)\n",
    "\n",
    "features = ['tenure_days', 'mrr', 'total_usage', 'features_adopted', 'total_events', 'usage_per_day', 'events_per_day']\n",
    "print(f\"Original features: {len(features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Curse of Dimensionality\n",
    "\n",
    "**Problem:** More features â‰  better predictions\n",
    "- Data becomes sparse\n",
    "- Computational complexity explodes\n",
    "- Noise becomes prominent\n",
    "- Can't visualize >3D\n",
    "\n",
    "**ðŸ’¡ Depth Note:** Test on high-dimensional datasets (200+ features). When does performance plateau or decrease?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "X = df[features]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Data shape: {X_scaled.shape}\")\n",
    "print(f\"Features: {X_scaled.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: PCA - Find Important Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA on full data\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPLAINED VARIANCE BY COMPONENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(len(pca_full.explained_variance_ratio_)):\n",
    "    var_pct = 100 * pca_full.explained_variance_ratio_[i]\n",
    "    cumsum_pct = 100 * cumsum_var[i]\n",
    "    print(f\"PC{i+1}: {var_pct:>5.1f}% | Cumulative: {cumsum_pct:>6.1f}%\")\n",
    "\n",
    "# Components needed for 95% variance\n",
    "n_comp_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "print(f\"\\nâœ… Components for 95% variance: {n_comp_95}\")\n",
    "print(f\"âœ… Dimensionality reduction: {len(features)} â†’ {n_comp_95} ({100*n_comp_95/len(features):.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Component Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 2D\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "var_2d = 100 * sum(pca_2d.explained_variance_ratio_)\n",
    "print(f\"2D PCA: {var_2d:.1f}% variance\")\n",
    "\n",
    "# PC1 interpretation\n",
    "print(\"\\nPC1 loadings (feature contributions):\")\n",
    "pc1_df = pd.DataFrame(\n",
    "    {'Feature': features, 'Loading': pca_2d.components_[0]}\n",
    ").sort_values('Loading', ascending=False)\n",
    "print(pc1_df.to_string(index=False))\n",
    "\n",
    "# PC2 interpretation\n",
    "print(\"\\nPC2 loadings (feature contributions):\")\n",
    "pc2_df = pd.DataFrame(\n",
    "    {'Feature': features, 'Loading': pca_2d.components_[1]}\n",
    ").sort_values('Loading', ascending=False)\n",
    "print(pc2_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ’¡ Business Interpretation:\")\n",
    "print(f\"   PC1: {pc1_df.iloc[0]['Feature']} + {pc1_df.iloc[1]['Feature']} (Engagement/Size)\")\n",
    "print(f\"   PC2: {pc2_df.iloc[0]['Feature']} vs {pc2_df.iloc[-1]['Feature']} (Growth patterns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Feature Importance from PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which original features drive all principal components?\n",
    "importance = np.sum(np.abs(pca_full.components_), axis=0)\n",
    "\n",
    "feature_imp = pd.DataFrame(\n",
    "    {'Feature': features, 'Importance': importance}\n",
    ").sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE IMPORTANCE IN PCA\")\n",
    "print(\"=\" * 60)\n",
    "print(feature_imp.to_string(index=False))\n",
    "\n",
    "print(f\"\\nâœ… Top features: {', '.join(feature_imp.head(3)['Feature'].tolist())}\")\n",
    "print(f\"âš ï¸ Least important: {feature_imp.iloc[-1]['Feature']} (redundant?)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: PCA for Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct using fewer components (removes noise)\n",
    "pca_denoise = PCA(n_components=3)\n",
    "X_reduced = pca_denoise.fit_transform(X_scaled)\n",
    "X_reconstructed = pca_denoise.inverse_transform(X_reduced)\n",
    "\n",
    "# Reconstruction error\n",
    "recon_error = np.mean(np.sum((X_scaled - X_reconstructed) ** 2, axis=1))\n",
    "\n",
    "print(f\"Reconstruction with 3 components:\")\n",
    "print(f\"  Variance retained: {100 * sum(pca_denoise.explained_variance_ratio_):.1f}%\")\n",
    "print(f\"  Reconstruction MSE: {recon_error:.4f}\")\n",
    "print(f\"  Information loss: {100 * (1 - sum(pca_denoise.explained_variance_ratio_)):.1f}%\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Denoising Trade-off:\")\n",
    "print(\"  More components = more information but more noise\")\n",
    "print(\"  Fewer components = cleaner data but information loss\")\n",
    "print(\"  Optimal: Find knee in reconstruction error vs components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Anomaly Detection via Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customers with high reconstruction error = anomalies\n",
    "recon_errors = np.sum((X_scaled - X_reconstructed) ** 2, axis=1)\n",
    "threshold = np.percentile(recon_errors, 95)\n",
    "\n",
    "anomalies = recon_errors > threshold\n",
    "n_anomalies = anomalies.sum()\n",
    "\n",
    "print(f\"\\nAnomaly Detection Results:\")\n",
    "print(f\"  Threshold (95th percentile): {threshold:.2f}\")\n",
    "print(f\"  Anomalous customers: {n_anomalies:,} ({100*n_anomalies/len(df):.1f}%)\")\n",
    "\n",
    "# Characterize anomalies\n",
    "print(\"\\nAnomalous customers (top 10 by error):\")\n",
    "anom_df = df.copy()\n",
    "anom_df['recon_error'] = recon_errors\n",
    "top_anomalies = anom_df.nlargest(10, 'recon_error')\n",
    "print(top_anomalies[['mrr', 'total_usage', 'tenure_days', 'recon_error']].to_string())\n",
    "\n",
    "print(\"\\nðŸ’¡ Business Insight:\")\n",
    "print(\"  These customers don't fit normal patterns\")\n",
    "print(\"  Could indicate: new customer type, usage anomaly, or data quality issue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On Exercises\n",
    "\n",
    "### Exercise 1: Scree Plot\n",
    "Create a plot of explained variance per component. Find the \"elbow\" where marginal improvement drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot explained variance per component\n",
    "# TODO: Find elbow point\n",
    "# TODO: Decision: How many components to keep?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: PCA + Clustering Comparison\n",
    "1. K-Means on original features\n",
    "2. K-Means on PCA-reduced features (2-3 components)\n",
    "3. Compare silhouette scores\n",
    "4. When does PCA help vs hurt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare clustering with/without PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Autoencoder Alternative\n",
    "Compare PCA (linear) to autoencoders (non-linear) for dimensionality reduction. Which preserves more relevant structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explore non-linear dimensionality reduction\n",
    "# from sklearn.manifold import TSNE, UMAP\n",
    "# Could explore t-SNE or UMAP for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: Visualization Dashboard\n",
    "\n",
    "**Build:**\n",
    "1. Scree plot (variance explained)\n",
    "2. Biplot (data points + feature loadings in 2D)\n",
    "3. Anomaly visualization (high reconstruction error customers)\n",
    "4. 3D scatter (PC1, PC2, PC3 colored by segment)\n",
    "\n",
    "**Insight:** What patterns emerge in reduced dimensional space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "âœ… Curse of dimensionality concept  \n",
    "âœ… PCA mechanics and interpretation  \n",
    "âœ… Variance explained by components  \n",
    "âœ… Feature importance from loadings  \n",
    "âœ… Visualization in lower dimensions  \n",
    "âœ… Denoising via reconstruction  \n",
    "âœ… Anomaly detection via reconstruction error  \n",
    "\n",
    "## ðŸ”œ Next Week: Ensemble Methods\n",
    "\n",
    "Combine multiple models for superior predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
