{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 10 \u2014 Ensemble Methods: Boosting & Bagging\n\n**Course:** Applied ML Foundations for SaaS Analytics  \n**Week Focus:** Combine multiple models for robust predictions.\n\n---\n\n## \ud83c\udfaf Learning Objectives\n\n- Understand ensemble methods: Bagging, Boosting, Stacking\n- Implement Gradient Boosting & XGBoost\n- Tune ensemble hyperparameters\n- Compare single vs ensemble model performance\n- Build production-grade ensemble pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd, numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score\n\n# Load & prepare data\nsubs = pd.read_csv('../data/subscriptions.csv')\nfeature_usage = pd.read_csv('../data/feature_usage.csv')\nuser_events = pd.read_csv('../data/user_events.csv')\n\n# Quick feature engineering\nengagement = feature_usage.groupby('user_id').agg({'usage_count': 'sum', 'feature_name': 'nunique'}).reset_index()\nengagement.columns = ['user_id', 'total_usage', 'features_adopted']\nevents = user_events.groupby('user_id').size().reset_index(name='total_events')\n\ndf = subs[['user_id', 'tenure_days', 'mrr', 'churn_date']].merge(engagement, on='user_id', how='left')\ndf = df.merge(events, on='user_id', how='left').fillna(0)\ndf['churned'] = df['churn_date'].notna().astype(int)\n\nX = df[['tenure_days', 'mrr', 'total_usage', 'features_adopted', 'total_events']]\ny = df['churned']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Dataset: {len(df)} customers | {y.sum()} churned ({100*y.mean():.1f}%)\")\nprint(f\"Train: {len(X_train)} | Test: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Bagging vs Boosting\n\n**Bagging** (Random Forest): Train multiple models on random subsets. Average predictions. Reduces variance.\n\n**Boosting** (Gradient Boosting): Train sequentially, each corrects previous. Weight hard examples. Reduces bias.\n\n**\ud83d\udca1 Depth Note:** When does boosting outperform bagging? Compare on imbalanced datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bagging: Random Forest\nrf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\nrf.fit(X_train, y_train)\nrf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\n\n# Boosting: Gradient Boosting\ngb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\ngb.fit(X_train, y_train)\ngb_auc = roc_auc_score(y_test, gb.predict_proba(X_test)[:, 1])\n\nprint(\"=\"*60)\nprint(\"ENSEMBLE COMPARISON\")\nprint(\"=\"*60)\nprint(f\"Random Forest (Bagging) AUC: {rf_auc:.4f}\")\nprint(f\"Gradient Boosting AUC:       {gb_auc:.4f}\")\nprint(f\"Winner: {'Boosting' if gb_auc > rf_auc else 'Bagging'} (+{abs(gb_auc - rf_auc):.4f})\")\n\nprint(f\"\\nTop features (GB):\")\nfeatures = X.columns\nfor feat, imp in sorted(zip(features, gb.feature_importances_), key=lambda x: x[1], reverse=True)[:3]:\n    print(f\"  {feat:.<25} {imp:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Voting Classifier (Stacking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine 3 different models\nvoting = VotingClassifier(\n    estimators=[\n        ('lr', LogisticRegression(random_state=42, max_iter=1000)),\n        ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n        ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42))\n    ],\n    voting='soft'\n)\nvoting.fit(X_train, y_train)\nvoting_auc = roc_auc_score(y_test, voting.predict_proba(X_test)[:, 1])\n\nprint(f\"Voting Classifier AUC: {voting_auc:.4f}\")\nprint(f\"Improvement over best single: {voting_auc - max(rf_auc, gb_auc):.4f}\")\n\nprint(\"\\n\ud83d\udca1 Ensemble combinations work best when:\")\nprint(\"  - Models have different architectures\")\nprint(\"  - Models make different types of errors\")\nprint(\"  - Diversity + accuracy balance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Hyperparameter Tuning\n\n**\ud83d\udca1 Depth Note:** Grid search over ensemble parameters. Trade-off between complexity and performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple parameter sweep\nlearning_rates = [0.01, 0.05, 0.1, 0.2]\nn_estimators_list = [50, 100, 200]\nresults = []\n\nfor lr in learning_rates:\n    for n in n_estimators_list:\n        gb_test = GradientBoostingClassifier(learning_rate=lr, n_estimators=n, max_depth=5, random_state=42)\n        gb_test.fit(X_train, y_train)\n        auc = roc_auc_score(y_test, gb_test.predict_proba(X_test)[:, 1])\n        results.append({'LR': lr, 'N': n, 'AUC': auc})\n\nresults_df = pd.DataFrame(results)\nbest = results_df.loc[results_df['AUC'].idxmax()]\nprint(f\"Best hyperparameters:\")\nprint(f\"  Learning Rate: {best['LR']}\")\nprint(f\"  N Estimators: {int(best['N'])}\")\nprint(f\"  AUC: {best['AUC']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hands-On Exercises\n\n### Exercise 1: XGBoost Implementation\nCompare XGBoost vs Gradient Boosting. Is faster training worth accuracy loss?\n\n### Exercise 2: Stacking Levels\nBuild a 2-level stacker: base models \u2192 meta-learner. How much improvement?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement XGBoost\n# TODO: Build 2-level stacking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n\nTakeaway checklist for this week...\n\n## \ud83d\udd1c Next Week: Deep Learning Fundamentals"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}